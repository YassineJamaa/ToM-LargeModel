{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from tom_localizer import ImportLLMfromHF, ToMLocalizerUnits, ToMLocDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from prompt_processing import process_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the environment\n",
    "# Load the variable from .env\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49059105a13049a28c7dcf3b9fdd9577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM Model\n",
    "checkpoint =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# Load the LLM Model and the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMLocDataset:\n",
    "    \"\"\" \n",
    "    ToM Dataset for Neural Units Localization \n",
    "       \"\"\"\n",
    "    def __init__(self):\n",
    "        loc_dir = \"dataset/prompt/tom-loc\"\n",
    "        self.fb_stories = [process_text_file(f\"{loc_dir}/{idx}b_story_question.txt\") for idx in range(1,11)]\n",
    "        self.pb_stories = [process_text_file(f\"{loc_dir}/{idx}p_story_question.txt\") for idx in range(1,11)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.fb_stories[idx].strip(), self.pb_stories[idx].strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fb_stories)\n",
    "\n",
    "tom = ToMLocDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportLLMfromHF:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_embd_size(self):\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def get_nb_layers(self):\n",
    "        return self.model.config.num_hidden_layers\n",
    "\n",
    "class ToMLayersUnits:\n",
    "    def __init__(self,\n",
    "                llm: ImportLLMfromHF,\n",
    "                tom_data: ToMLocDataset,\n",
    "                 ):\n",
    "        self.llm = llm\n",
    "        self.tom_data = tom_data\n",
    "        self.data_activation = None\n",
    "        self.group = {\"false-belief\": 0, \"false-photo\": 1}\n",
    "        self.extract_all_units()\n",
    "    \n",
    "    def reset_data_activation(self):\n",
    "        \"\"\"Initialize or reset the data activation tensor.\"\"\"\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        self.data_activation = torch.zeros(2, len(self.tom_data), embd_size, n_layers)\n",
    "    \n",
    "    # Function to remove all hooks from model layers\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear hooks and reset activations for safe re-initialization.\"\"\"\n",
    "        self.clear_hooks()\n",
    "        self.reset_data_activation()\n",
    "    \n",
    "    # Define the function to capture the output of each layer with hooks\n",
    "    def get_hook_layers(self, idx, activation):\n",
    "        def hook_layers(module, input, output):\n",
    "            activation[:,:,:, idx] = output[0].squeeze(0)\n",
    "        return hook_layers\n",
    "    \n",
    "    def average_tokens_layers(self, activation):\n",
    "        # Calculate the average activation across tokens for each layer\n",
    "        # Resulting shape: (embd_size, n_layers)\n",
    "        avg_activation = activation.mean(dim=1)  # Average over tokens\n",
    "        return avg_activation  # Remove batch dimension if unnecessary\n",
    "\n",
    "    def extract_layer_units(self, idx, group_name=\"false-belief\"):\n",
    "        # Clear any previously registered hooks\n",
    "        self.clear_hooks()\n",
    "        # Process and tokenize the input text\n",
    "        prompt = self.tom_data[idx][self.group[group_name]]\n",
    "        inputs = self.llm.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        n_tokens = inputs[\"input_ids\"].shape[1] # Number of token in the input\n",
    "        embd_size = self.llm.get_embd_size() # Embedding size of the model\n",
    "        n_layers = self.llm.get_nb_layers() # Number of layers in the model\n",
    "        # Initialize the tensor to store activations: (batch_size=1, n_tokens, embd_size, n_layers)\n",
    "        activation = torch.zeros(1, n_tokens, embd_size, n_layers)\n",
    "\n",
    "        # Register hooks on each layer, passing activation as an argument\n",
    "        for i, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_layers(i, activation))\n",
    "        \n",
    "        # Pass the input through the model\n",
    "        with torch.no_grad():\n",
    "            self.llm.model(**inputs)\n",
    "        return self.average_tokens_layers(activation)\n",
    "    \n",
    "    def extract_all_units(self):\n",
    "        \"\"\"Extract activations for all items in the dataset.\"\"\"\n",
    "        self.reset()  # Ensure clean state before extraction\n",
    "        for idx in range(len(self.tom_data)):\n",
    "            # Extract and store activation data in \"False-Belief\" group\n",
    "            self.data_activation[0, idx, :, :] = self.extract_layer_units(idx, \"false-belief\")\n",
    "            # Extract and store activation data in \"False-Photo\" group\n",
    "            self.data_activation[1, idx, :, :] = self.extract_layer_units(idx, \"false-photo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LocImportantUnits:\n",
    "    def __init__(self,\n",
    "                 checkpoint,\n",
    "                 layers_units: torch.Tensor):\n",
    "        self.model_name = checkpoint.split(\"/\")[-1]\n",
    "        self.fb_group = layers_units[0]\n",
    "        self.fp_group = layers_units[1]\n",
    "        self.t_values = self.welch_test()\n",
    "        self.ranked_units = self.get_ranked_units()\n",
    "\n",
    "    def welch_test(self):\n",
    "        n_units = self.fb_group.shape[1]\n",
    "        n_layers = self.fb_group.shape[2]\n",
    "\n",
    "        # Reshape for Welch t-test\n",
    "        fb_flattened = self.fb_group.reshape(self.fb_group.shape[0], -1)\n",
    "        fp_flattened = self.fp_group.reshape(self.fp_group.shape[0], -1)\n",
    "\n",
    "        # Perform the t-test along the first axis (sample dimension)\n",
    "        t_stat, _ = ttest_ind(fb_flattened, fp_flattened, axis=0, equal_var=False)\n",
    "\n",
    "        # Reshape t_stat back to (units, n_layers)\n",
    "        return t_stat.reshape(n_units, n_layers)\n",
    "    \n",
    "    def get_ranked_units(self):\n",
    "        # Get ranked matrix\n",
    "        flat = self.t_values.flatten()\n",
    "        sorted_indices = np.argsort(flat)[::-1]  # Sort indices in descending order\n",
    "        ranked = np.empty_like(sorted_indices)\n",
    "        ranked[sorted_indices] = np.arange(1, len(flat) + 1)\n",
    "        # Reshape the ranked values back to the original matrix shape\n",
    "        return ranked.reshape(self.t_values.shape)\n",
    "    \n",
    "    def get_masked_ktop(self, percentage):\n",
    "        num_top_elements = int(self.t_values.size * percentage)\n",
    "        # Flatten the matrix, find the threshold value for the top 1%\n",
    "        flattened_matrix = self.t_values.flatten()\n",
    "        threshold_value = np.partition(flattened_matrix, -num_top_elements)[-num_top_elements]\n",
    "\n",
    "        # Create a binary mask where 1 represents the top 1% elements, and 0 otherwise\n",
    "        mask_units = np.where(self.t_values >= threshold_value, 1, 0)\n",
    "        return mask_units\n",
    "    \n",
    "    def get_random_mask(self, percentage, seed=None):\n",
    "        # Set the seed for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Calculate the total number of units\n",
    "        total_units = self.t_values.size\n",
    "        num_units_to_select = int(total_units * percentage)\n",
    "        \n",
    "        # Create a flattened array of zeros\n",
    "        mask_flat = np.zeros(total_units, dtype=int)\n",
    "        \n",
    "        # Randomly select indices and set them to 1\n",
    "        selected_indices = np.random.choice(total_units, num_units_to_select, replace=False)\n",
    "        mask_flat[selected_indices] = 1\n",
    "        \n",
    "        # Reshape the mask back to the original shape\n",
    "        return mask_flat.reshape(self.t_values.shape)\n",
    "    \n",
    "    def plot_layer_percentages(self, percentage, mask_type='ktop', seed=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plots the percentage of important units per layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - percentage (float): The top percentage of units to be considered as important.\n",
    "        - mask_type (str): Type of mask to use ('ktop' for k-top mask or 'random' for random mask).\n",
    "        - seed (int, optional): Random seed for reproducibility when using the random mask.\n",
    "        - save_path (str, optional): Path to save the plot. If None, shows the plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the mask based on the specified mask type\n",
    "        if mask_type == 'ktop':\n",
    "            mask = self.get_masked_ktop(percentage)\n",
    "        elif mask_type == 'random':\n",
    "            mask = self.get_random_mask(percentage, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask_type. Choose 'ktop' or 'random'.\")\n",
    "        \n",
    "        # Calculate the percentage of important units for each layer\n",
    "        layer_percentages = [(np.sum(layer) / layer.size) * 100 for layer in mask.T]\n",
    "\n",
    "        # Convert to a column vector for plotting\n",
    "        layer_percentages_matrix = np.array(layer_percentages).reshape(-1, 1)\n",
    "\n",
    "        # Plot the layer percentages as a matrix with shape (number of layers, 1)\n",
    "        plt.figure(figsize=(2, 8))\n",
    "        plt.imshow(layer_percentages_matrix, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(label=\"Percentage of Important Units\")\n",
    "        plt.title(f\"Percentage of Important Units per Layer ({mask_type.capitalize()} Mask, Top {percentage*100:.1f}%)\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "\n",
    "        # Add text annotations for each percentage\n",
    "        for i, perc in enumerate(layer_percentages):\n",
    "            plt.text(0, i, f\"{perc:.2f}%\", ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if perc < 1.3 else \"black\")\n",
    "\n",
    "        # Configure ticks\n",
    "        plt.yticks(range(len(layer_percentages)), [f\"Layer {i+1}\" for i in range(len(layer_percentages))])\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Save the plot or show it based on the save_path parameter\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class AblateUnits:\n",
    "    def __init__(self,\n",
    "                 llm: ImportLLMfromHF,\n",
    "                 mask: Optional[np.ndarray] = None):\n",
    "        self.llm = llm\n",
    "        self.mask = mask\n",
    "        self.layer_outputs = []\n",
    "    \n",
    "    # Function to remove all hooks from model layers\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def get_hook_ablate(self, idx):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = self.mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "            self.layer_outputs.append(output[0].clone())\n",
    "        return hook_ablate\n",
    "    \n",
    "    def ablate_units(self, prompt):\n",
    "        self.clear_hooks()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        # Register hooks on each layer, passing activation as an argument\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_tokens = self.llm.model.generate(**inputs, max_length=50, do_sample=True, top_p=0.95, top_k=50)\n",
    "    \n",
    "        # Decode generated tokens to string\n",
    "        decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and localizer classes only once per session\n",
    "tom_data = ToMLocDataset()\n",
    "llm = ImportLLMfromHF(model, tokenizer)\n",
    "tom_units = ToMLayersUnits(llm, tom_data)\n",
    "loc_units = LocImportantUnits(checkpoint, tom_units.data_activation)\n",
    "mask_random = loc_units.get_random_mask(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? I hope you're doing great!\\n\\nI've been thinking about how to make the blog more interactive and engaging for readers, and I thought I'd reach out to you for some ideas.\\n\\nHere are a few\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation = AblateUnits(llm, mask_random.T)\n",
    "perturbation.ablate_units(\"Hello! How are you doing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
