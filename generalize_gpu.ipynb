{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from prompt_processing import process_text_file\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialization of the environment\n",
    "# Load the variable from .env\n",
    "pd.set_option('display.max_columns', None)\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38638ad491454745852c2e9718768a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer onto GPU\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportLLMfromHF:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_embd_size(self):\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def get_nb_layers(self):\n",
    "        return self.model.config.num_hidden_layers\n",
    "\n",
    "class LayersUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, data: Dataset, method: str = \"average\"):\n",
    "        self.llm = llm\n",
    "        self.data = data\n",
    "        self.data_activation = None\n",
    "        self.group = {\"positive\": 0, \"negative\": 1}\n",
    "        self.method_fn = self.average_tokens_layers if method == \"average\" else self.final_tokens_layers if method == \"final\" else None\n",
    "        self.name = type(data).__name__\n",
    "\n",
    "        # Set tokenizer arguments based on dataset name\n",
    "        self.tokenizer_args = {\"return_tensors\": \"pt\"}\n",
    "        if self.name == \"LangLocDataset\":\n",
    "            self.tokenizer_args.update({\"truncation\": True, \"max_length\": 12})\n",
    "\n",
    "        self.extract_all_units()\n",
    "\n",
    "    def reset_data_activation(self):\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        # Move the tensor to GPU\n",
    "        self.data_activation = torch.zeros(2, len(self.data), embd_size, n_layers, device=device)\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.clear_hooks()\n",
    "        self.reset_data_activation()\n",
    "\n",
    "    def get_hook_layers(self, idx, activation):\n",
    "        def hook_layers(module, input, output):\n",
    "            activation[:, :, :, idx] = output[0].squeeze(0).to(device)\n",
    "        return hook_layers\n",
    "\n",
    "    def average_tokens_layers(self, activation):\n",
    "        return activation.mean(dim=1)\n",
    "\n",
    "    def final_tokens_layers(self, activation):\n",
    "        return activation[:, -1, :, :]\n",
    "\n",
    "    def extract_layer_units(self, idx, group_name=\"positive\", method=\"average\"):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "\n",
    "        prompt = self.data[idx][self.group[group_name]]\n",
    "        inputs = self.llm.tokenizer(prompt, **self.tokenizer_args).to(device)\n",
    "        \n",
    "        n_tokens = inputs[\"input_ids\"].shape[1]\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        \n",
    "        activation = torch.zeros(1, n_tokens, embd_size, n_layers, device=device)\n",
    "\n",
    "        for i, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_layers(i, activation))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.llm.model(**inputs)\n",
    "\n",
    "        return self.method_fn(activation)\n",
    "\n",
    "    def extract_all_units(self):\n",
    "        self.reset()\n",
    "        for idx in range(len(self.data)):\n",
    "            self.data_activation[0, idx, :, :] = self.extract_layer_units(idx, \"positive\")\n",
    "            self.data_activation[1, idx, :, :] = self.extract_layer_units(idx, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMLocDataset(Dataset):\n",
    "    \"\"\" \n",
    "    ToM Dataset for Neural Units Localization \n",
    "       \"\"\"\n",
    "    def __init__(self):\n",
    "        loc_dir = \"dataset/prompt/tom-loc\"\n",
    "        self.positive = [process_text_file(f\"{loc_dir}/{idx}b_story_question.txt\") for idx in range(1,11)]\n",
    "        self.negative = [process_text_file(f\"{loc_dir}/{idx}p_story_question.txt\") for idx in range(1,11)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "\n",
    "class LangLocDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dirpath = \"dataset/prompt/langloc\"\n",
    "        paths = glob(f\"{dirpath}/*.csv\")\n",
    "        vocab = set()\n",
    "\n",
    "        data = pd.read_csv(paths[0])\n",
    "        for path in paths[1:]:\n",
    "            run_data = pd.read_csv(path)\n",
    "            data = pd.concat([data, run_data])\n",
    "\n",
    "        data[\"sent\"] = data[\"stim2\"].apply(str.lower)\n",
    "\n",
    "        vocab.update(data[\"stim2\"].apply(str.lower).tolist())\n",
    "        for stimuli_idx in range(3, 14):\n",
    "            data[\"sent\"] += \" \" + data[f\"stim{stimuli_idx}\"].apply(str.lower)\n",
    "            vocab.update(data[f\"stim{stimuli_idx}\"].apply(str.lower).tolist())\n",
    "\n",
    "        self.vocab = sorted(list(vocab))\n",
    "        self.w2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2w = {i: w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "        self.positive = data[data[\"stim14\"]==\"S\"][\"sent\"].tolist()\n",
    "        self.negative = data[data[\"stim14\"]==\"N\"][\"sent\"].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocImportantUnits:\n",
    "    def __init__(self,\n",
    "                 checkpoint,\n",
    "                 layers_units: torch.Tensor):\n",
    "        self.model_name = checkpoint.split(\"/\")[-1]\n",
    "        self.fb_group = layers_units[0].cpu()\n",
    "        self.fp_group = layers_units[1].cpu()\n",
    "        self.t_values = self.welch_test()\n",
    "        self.ranked_units = self.get_ranked_units()\n",
    "\n",
    "    def welch_test(self):\n",
    "        n_units = self.fb_group.shape[1]\n",
    "        n_layers = self.fb_group.shape[2]\n",
    "\n",
    "        # Reshape for Welch t-test\n",
    "        fb_flattened = np.abs(self.fb_group.reshape(self.fb_group.shape[0], -1))\n",
    "        fp_flattened = np.abs(self.fp_group.reshape(self.fp_group.shape[0], -1))\n",
    "        # Perform the t-test along the first axis (sample dimension)\n",
    "        t_stat, _ = ttest_ind(fb_flattened, fp_flattened, axis=0, equal_var=False)\n",
    "        print(t_stat.shape)\n",
    "\n",
    "        # Reshape t_stat back to (units, n_layers)\n",
    "        return t_stat.reshape(n_units, n_layers)\n",
    "    \n",
    "    def get_ranked_units(self):\n",
    "        # Get ranked matrix\n",
    "        flat = self.t_values.flatten()\n",
    "        sorted_indices = np.argsort(flat)[::-1]  # Sort indices in descending order\n",
    "        ranked = np.empty_like(sorted_indices)\n",
    "        ranked[sorted_indices] = np.arange(1, len(flat) + 1)\n",
    "        # Reshape the ranked values back to the original matrix shape\n",
    "        return ranked.reshape(self.t_values.shape)\n",
    "    \n",
    "    def get_masked_ktop(self, percentage):\n",
    "        num_top_elements = int(self.t_values.size * percentage)\n",
    "        # Flatten the matrix, find the threshold value for the top 1%\n",
    "        flattened_matrix = self.t_values.flatten()\n",
    "        threshold_value = np.partition(flattened_matrix, -num_top_elements)[-num_top_elements]\n",
    "\n",
    "        # Create a binary mask where 1 represents the top 1% elements, and 0 otherwise\n",
    "        mask_units = np.where(self.t_values >= threshold_value, 1, 0)\n",
    "        return mask_units\n",
    "    \n",
    "    def get_random_mask(self, percentage, seed=None):\n",
    "        # Set the seed for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Calculate the total number of units\n",
    "        total_units = self.t_values.size\n",
    "        num_units_to_select = int(total_units * percentage)\n",
    "        \n",
    "        # Create a flattened array of zeros\n",
    "        mask_flat = np.zeros(total_units, dtype=int)\n",
    "        \n",
    "        # Randomly select indices and set them to 1\n",
    "        selected_indices = np.random.choice(total_units, num_units_to_select, replace=False)\n",
    "        mask_flat[selected_indices] = 1\n",
    "        \n",
    "        # Reshape the mask back to the original shape\n",
    "        return mask_flat.reshape(self.t_values.shape)\n",
    "    \n",
    "    def plot_layer_percentages(self, percentage, mask_type='ktop', seed=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plots the percentage of important units per layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - percentage (float): The top percentage of units to be considered as important.\n",
    "        - mask_type (str): Type of mask to use ('ktop' for k-top mask or 'random' for random mask).\n",
    "        - seed (int, optional): Random seed for reproducibility when using the random mask.\n",
    "        - save_path (str, optional): Path to save the plot. If None, shows the plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the mask based on the specified mask type\n",
    "        if mask_type == 'ktop':\n",
    "            mask = self.get_masked_ktop(percentage)\n",
    "        elif mask_type == 'random':\n",
    "            mask = self.get_random_mask(percentage, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask_type. Choose 'ktop' or 'random'.\")\n",
    "        \n",
    "        # Calculate the percentage of important units for each layer\n",
    "        layer_percentages = [(np.sum(layer) / layer.size) * 100 for layer in mask.T]\n",
    "\n",
    "        # Convert to a column vector for plotting\n",
    "        layer_percentages_matrix = np.array(layer_percentages).reshape(-1, 1)\n",
    "\n",
    "        # Plot the layer percentages as a matrix with shape (number of layers, 1)\n",
    "        plt.figure(figsize=(2, 8))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        norm = Normalize(vmin=min(layer_percentages), vmax=max(layer_percentages))\n",
    "        plt.imshow(layer_percentages_matrix, cmap=cmap, aspect='auto')\n",
    "        plt.colorbar(label=\"Percentage of Important Units\")\n",
    "        plt.title(f\"Percentage of Important Units per Layer ({mask_type.capitalize()} Mask, Top {percentage*100:.1f}%)\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "\n",
    "        # Add text annotations with adaptive color based on background brightness\n",
    "        for i, perc in enumerate(layer_percentages):\n",
    "            color = cmap(norm(perc))\n",
    "            brightness = 0.3 * color[0] + 0.5 * color[1] + 0.2 * color[2]\n",
    "            text_color = \"white\" if brightness < 0.5 else \"black\"\n",
    "            plt.text(0, i, f\"{perc:.1f}%\", ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "        # Configure ticks\n",
    "        plt.yticks(range(len(layer_percentages)), [f\"Layer {i+1}\" for i in range(len(layer_percentages))])\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Save the plot or show it based on the save_path parameter\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblateUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, mask: Optional[np.ndarray] = None):\n",
    "        self.llm = llm\n",
    "        self.mask = mask\n",
    "        self.layer_outputs = []\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = self.mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "            self.layer_outputs.append(output[0].clone().to(device))\n",
    "        return hook_ablate\n",
    "\n",
    "    def ablate_units(self, prompt):\n",
    "        self.clear_hooks()\n",
    "        self.layer_outputs.clear()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = self.llm.model.generate(**inputs, max_length=50, do_sample=True, top_p=0.95, top_k=50)\n",
    "\n",
    "        decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131072,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2805292/2362297561.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647329220/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  output[0][:,:,unit_indices] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! What it the capital of France?\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model and localizer classes only once per session\n",
    "lang_data = LangLocDataset()\n",
    "tom_data = ToMLocDataset()\n",
    "llm = ImportLLMfromHF(model, tokenizer)\n",
    "units = LayersUnits(llm, lang_data, \"final\")\n",
    "loc_units = LocImportantUnits(checkpoint, units.data_activation)\n",
    "mask = loc_units.get_masked_ktop(0.01)\n",
    "perturbation = AblateUnits(llm, mask.T)\n",
    "perturbation.ablate_units(\"Hello! What it the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateBenchmark:\n",
    "    def __init__(self,\n",
    "                llm: ImportLLMfromHF,\n",
    "                loc_units: LocImportantUnits,\n",
    "                batch_size: int=20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "        return hook_ablate\n",
    "    \n",
    "    def get_generated_tokens(self, outputs, input_length):\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            new_tokens = output[input_length:]  # Exclude prompt tokens\n",
    "            generated_texts.append(tokenizer.decode(new_tokens, skip_special_tokens=True))\n",
    "        return generated_texts\n",
    "\n",
    "    def generate_text_with_ablations(self, prompts, mask, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "    \n",
    "    def generate_text_without_ablation(self, prompts, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "        inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "     \n",
    "    def generate(self, prompts, mask=None):\n",
    "        # Set temperature to 0 for deterministic generation\n",
    "        generation_config = {\n",
    "            \"temperature\": None,          # Explicitly unset temperature\n",
    "            \"top_p\": None,   \n",
    "            \"do_sample\": False,  # Ensures deterministic generation\n",
    "            \"pad_token_id\": tokenizer.eos_token_id  # Set the padding token if necessary\n",
    "        }\n",
    "\n",
    "        if mask is None:\n",
    "            return self.generate_text_without_ablation(prompts, **generation_config)\n",
    "        else:\n",
    "            return self.generate_text_with_ablations(prompts, mask, **generation_config)\n",
    "        \n",
    "    def compute_surprisal(self, prompt: str, candidates: list):\n",
    "        \"\"\"\n",
    "        Computes surprisal values for a list of candidate completions given a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The prompt for the language model.\n",
    "            candidates (list): A list of candidate completions.\n",
    "            llm (ImportLLMfromHF): Model and tokenizer wrapper.\n",
    "\n",
    "        Returns:\n",
    "            str: The candidate with the lowest surprisal score.\n",
    "        \"\"\"\n",
    "        # Determine device (GPU if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        candidates_surprisal = []\n",
    "        prompt_inputs = self.llm.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            # Tokenize candidate\n",
    "            candidate_inputs = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Concatenate prompt and candidate tokens\n",
    "            input_ids = torch.cat([prompt_inputs[\"input_ids\"], candidate_inputs[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "            # Get logits from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = llm.model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Extract logits for the candidate tokens\n",
    "            start_idx = prompt_inputs[\"input_ids\"].shape[1]\n",
    "            candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "            # Calculate negative log-probability (surprisal) for each candidate token\n",
    "            candidate_logprobs = []\n",
    "            for i, token_id in enumerate(candidate_inputs[\"input_ids\"][0, 1:]):\n",
    "                token_logits = candidate_logits[i]\n",
    "                token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                candidate_logprobs.append(-token_logprob.item())  # negative log-probability as surprisal\n",
    "\n",
    "            # Average surprisal for the candidate\n",
    "            avg_surprisal = np.mean(candidate_logprobs)\n",
    "            candidates_surprisal.append(avg_surprisal)\n",
    "\n",
    "        # Find the candidate with the lowest surprisal score\n",
    "        min_surprisal_index = np.argmin(candidates_surprisal)\n",
    "        return candidates[min_surprisal_index]\n",
    "    \n",
    "    \n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"  \"\"\"\n",
    "        data = df.copy()\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            f\"ablate_top_{pct*100}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            f\"ablate_random1_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random2_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random3_{pct*100}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        for key, mask in assess_dict.items():\n",
    "            # Generate responses in batches and collect results\n",
    "            predict_cands = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size)):\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                predict_cands.extend(self.generate(batch_prompts, mask))\n",
    "            \n",
    "            data[f\"predict_{key}\"] = predict_cands\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AssessBenchmark:\n",
    "    def __init__(self,\n",
    "                 llm: ImportLLMfromHF,\n",
    "                 loc_units: LocImportantUnits,\n",
    "                 batch_size: int = 20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Clears all registered forward hooks in the model layers.\"\"\"\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        \"\"\"\n",
    "        Defines a hook function to ablate specific units based on a mask.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Layer index.\n",
    "            mask (torch.Tensor): Binary mask for ablation at the given layer.\n",
    "\n",
    "        Returns:\n",
    "            function: A hook function to zero out specified units.\n",
    "        \"\"\"\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:, :, unit_indices] = 0\n",
    "        return hook_ablate\n",
    "\n",
    "    def surprisal(self, prompts, candidates_list, mask=None):\n",
    "        \"\"\"\n",
    "        Computes surprisal for each candidate completion in a batch of prompts and returns\n",
    "        the candidate with the lowest surprisal for each prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompts (list): List of prompt strings.\n",
    "            candidates_list (list of lists): List of candidate lists for each prompt.\n",
    "            mask (optional): Mask for ablation.\n",
    "\n",
    "        Returns:\n",
    "            list: A list where each element is the candidate with the lowest surprisal for each prompt.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        if mask is not None:\n",
    "            for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "                layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        # Tokenize prompts in a batch\n",
    "        prompt_inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        surprisal_results = []\n",
    "\n",
    "        for prompt_input, candidates in zip(prompt_inputs[\"input_ids\"], candidates_list):\n",
    "            candidate_surprisals = []\n",
    "\n",
    "            # Process each candidate individually for the current prompt\n",
    "            for candidate in candidates:\n",
    "                candidate_input = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "                # Concatenate the prompt and candidate tokens\n",
    "                input_ids = torch.cat([prompt_input.unsqueeze(0), candidate_input[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.llm.model(input_ids)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Extract logits for the candidate tokens\n",
    "                start_idx = prompt_input.shape[0]\n",
    "                candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "                # Calculate negative log-probability (surprisal) for each token in the candidate\n",
    "                candidate_logprobs = []\n",
    "                for i, token_id in enumerate(candidate_input[\"input_ids\"][0, 1:]):\n",
    "                    token_logits = candidate_logits[i]\n",
    "                    token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                    candidate_logprobs.append(-token_logprob.item())\n",
    "\n",
    "                # Calculate average surprisal for the candidate and store it\n",
    "                avg_surprisal = np.mean(candidate_logprobs)\n",
    "                candidate_surprisals.append(avg_surprisal)\n",
    "\n",
    "            # Find the index of the candidate with the lowest surprisal\n",
    "            min_index = np.argmin(candidate_surprisals)\n",
    "            surprisal_results.append(candidates[min_index])\n",
    "\n",
    "        self.clear_hooks()  # Clear hooks after computation\n",
    "        return surprisal_results\n",
    "\n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"\n",
    "        Run an experiment on a DataFrame of prompts to evaluate surprisal scores\n",
    "        with and without ablations.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame containing the prompts and candidates.\n",
    "            pct (float): The percentage of top units to ablate in some settings.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The original DataFrame with added columns for surprisal scores.\n",
    "        \"\"\"\n",
    "        data = df.copy()\n",
    "        \n",
    "        # Define different ablation masks for comparison\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            # f\"ablate_top_{int(pct * 100)}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            # f\"ablate_random1_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random2_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random3_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        # For each ablation setting, compute surprisal scores\n",
    "        for key, mask in assess_dict.items():\n",
    "            surprisal_scores = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size), desc=f\"Processing {key}\"):\n",
    "                # Fetch a batch of prompts and corresponding candidates\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                batch_candidates = data[\"cands\"].iloc[i:i+self.batch_size].tolist()\n",
    "                \n",
    "                # Calculate surprisal scores for each prompt and its list of candidates\n",
    "                batch_scores = self.surprisal(batch_prompts, batch_candidates, mask)\n",
    "                surprisal_scores.extend(batch_scores)\n",
    "            \n",
    "            # Store the surprisal scores for each ablation setting\n",
    "            data[f\"predict_{key}\"] = surprisal_scores\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing no_ablation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing no_ablation: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "csv_file=\"dataset/benchmarks/ToMi/ToMi-finalNeuralTOM.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df[df[\"qOrder\"]=='first_order'].reset_index(drop=True)\n",
    "# Convert the 'cands' column from string representation of lists to actual lists\n",
    "df[\"cands\"] = df[\"cands\"].apply(ast.literal_eval)\n",
    "\n",
    "intro_text = (\n",
    "    \"The following multiple choice question is based on the following story. The question \"\n",
    "    \"is related to Theory-of-Mind. Read the story and then answer the questions. Choose the best answer \"\n",
    "    \"from the options provided by printing it as is without any modifications.\"\n",
    ")\n",
    "\n",
    "df[\"prompt\"] = df.apply(\n",
    "    lambda row: f\"{intro_text}\\n\\nStory:\\n{row['story']}\\n\\nQuestion:\\n{row['question']}\\nChoose between the following options: {row['cands'][0]} or {row['cands'][1]}. The response should be contained in your first sentence.\",\n",
    "    axis=1\n",
    ")\n",
    "data = df.iloc[:20]\n",
    "assess = AssessBenchmark(llm, loc_units, 20)\n",
    "res = assess.experiment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame (assuming data is the first 20 rows of df)\n",
    "data = df.copy()\n",
    "torch.cuda.empty_cache()\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame({\n",
    "    \"id_prompt\": data.index.repeat(data[\"cands\"].str.len()),\n",
    "    \"id_cand\": [i for sublist in data[\"cands\"] for i in range(len(sublist))],\n",
    "    \"num_tokens\": [len(tokenizer.tokenize(cand)) for cands in data[\"cands\"] for cand in cands],\n",
    "    \"cand\": [cand for cands in data[\"cands\"] for cand in cands],\n",
    "    \"fusion\": [f\"{prompt} {cand}\" for prompt, cands in zip(data[\"prompt\"], data[\"cands\"]) for cand in cands]\n",
    "})\n",
    "\n",
    "batch_size = 20\n",
    "# Initialize an empty list to store all processed group DataFrames\n",
    "all_group_embeddings = []\n",
    "for num_tokens, group_df in expanded_df.groupby(\"num_tokens\"):\n",
    "    all_last_token_embeddings = []  # Reset for each group\n",
    "    for i in range(0, len(group_df), batch_size):\n",
    "        batch_texts = group_df[\"fusion\"].tolist()[i:i+batch_size]\n",
    "        # Tokenize the batch of texts and get input IDs\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "    #     # Pass the batch through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "    #     # Calculate the position of the last token for each sequence in the batch\n",
    "        last_token_positions = (inputs[\"attention_mask\"].sum(dim=1) - 1)\n",
    "\n",
    "    #     # Define a range of offsets for the last 3 tokens\n",
    "        expanded_positions = last_token_positions[:, None] - torch.arange(num_tokens-1, -1, -1, device=device)\n",
    "        token_positions = torch.gather(inputs[\"input_ids\"], 1, expanded_positions)\n",
    "        # Reshape for indexing into `outputs.logits`\n",
    "        batch_indices = torch.arange(inputs[\"input_ids\"].shape[0]).unsqueeze(1).expand(-1, num_tokens)  # Shape: [20, 3]\n",
    "        batch_logits = outputs.logits[batch_indices, expanded_positions, token_positions]\n",
    "        all_last_token_embeddings.extend(batch_logits.cpu().numpy())\n",
    "\n",
    "    # Store the embeddings as a new column in the group DataFrame\n",
    "    group_df[\"token_logits\"] = all_last_token_embeddings\n",
    "    \n",
    "    # Append the processed group to the main list\n",
    "    all_group_embeddings.append(group_df)\n",
    "\n",
    "# Concatenate all groups back into a single DataFrame with token_logits column\n",
    "final_df = pd.concat(all_group_embeddings, ignore_index=False)\n",
    "# Merge token_logits back to the original expanded_df by index\n",
    "merge_df = expanded_df.join(final_df[[\"token_logits\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_prompt</th>\n",
       "      <th>id_cand</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>cand</th>\n",
       "      <th>fusion</th>\n",
       "      <th>token_logits</th>\n",
       "      <th>valid_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[5.8649683, 6.4997687]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bucket</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[5.191267]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[4.1025853, 6.0016346]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>drawer</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[6.8435116]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>bottle</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[3.7100582, 4.9050756]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>617</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[5.480484, 5.2861347]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>618</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>bathtub</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[4.8973117, 8.589179]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>618</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>envelope</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[4.8973117, 5.509709]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>drawer</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[8.205498]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>envelope</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>[4.066341, 7.3772044]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_prompt  id_cand  num_tokens      cand  \\\n",
       "0             0        0           2    pantry   \n",
       "1             0        1           1    bucket   \n",
       "2             1        0           2    pantry   \n",
       "3             1        1           1    drawer   \n",
       "4             2        0           2    bottle   \n",
       "...         ...      ...         ...       ...   \n",
       "1235        617        1           2    pantry   \n",
       "1236        618        0           2   bathtub   \n",
       "1237        618        1           2  envelope   \n",
       "1238        619        0           1    drawer   \n",
       "1239        619        1           2  envelope   \n",
       "\n",
       "                                                 fusion  \\\n",
       "0     The following multiple choice question is base...   \n",
       "1     The following multiple choice question is base...   \n",
       "2     The following multiple choice question is base...   \n",
       "3     The following multiple choice question is base...   \n",
       "4     The following multiple choice question is base...   \n",
       "...                                                 ...   \n",
       "1235  The following multiple choice question is base...   \n",
       "1236  The following multiple choice question is base...   \n",
       "1237  The following multiple choice question is base...   \n",
       "1238  The following multiple choice question is base...   \n",
       "1239  The following multiple choice question is base...   \n",
       "\n",
       "                token_logits  valid_length  \n",
       "0     [5.8649683, 6.4997687]          True  \n",
       "1                 [5.191267]          True  \n",
       "2     [4.1025853, 6.0016346]          True  \n",
       "3                [6.8435116]          True  \n",
       "4     [3.7100582, 4.9050756]          True  \n",
       "...                      ...           ...  \n",
       "1235   [5.480484, 5.2861347]          True  \n",
       "1236   [4.8973117, 8.589179]          True  \n",
       "1237   [4.8973117, 5.509709]          True  \n",
       "1238              [8.205498]          True  \n",
       "1239   [4.066341, 7.3772044]          True  \n",
       "\n",
       "[1240 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_tokens, group_df \u001b[38;5;129;01min\u001b[39;00m expanded_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num_tokens)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans-lm/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1950\u001b[0m     )\n\u001b[0;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans-lm/lib/python3.12/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: 0'"
     ]
    }
   ],
   "source": [
    "for num_tokens, group_df in expanded_df.groupby(\"num_tokens\")[0]:\n",
    "    print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3]) torch.Size([20, 140])\n",
      "torch.Size([20, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs and position tensors\n",
    "inputs = torch.randn(20, 140)  # Shape: [20, 140]\n",
    "position = torch.randint(0, 140, (20, 3))  # Shape: [20, 3]\n",
    "\n",
    "# Gather elements from inputs based on position\n",
    "output = torch.gather(inputs, 1, position)\n",
    "\n",
    "print(position.shape, inputs.shape)\n",
    "\n",
    "print(output.shape)  # Should be [20, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m expanded_positions \u001b[38;5;241m=\u001b[39m last_token_positions[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract the logits for the specified indices\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m logits_extracted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[torch\u001b[38;5;241m.\u001b[39marange(outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))[:, \u001b[38;5;28;01mNone\u001b[39;00m], expanded_positions, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][torch\u001b[38;5;241m.\u001b[39marange(outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))[:, \u001b[38;5;28;01mNone\u001b[39;00m], last_token_positions]]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Move last_token_positions and the range tensor to the same device\n",
    "last_token_positions = last_token_positions.to(device)\n",
    "# Define the range to cover 'last_token_positions' and the next 3 elements\n",
    "expanded_positions = last_token_positions[:, None] - torch.arange(2,-1,-1, device=device)\n",
    "\n",
    "# Extract the logits for the specified indices\n",
    "logits_extracted = outputs.logits[torch.arange(outputs.logits.size(0))[:, None], expanded_positions, inputs[\"input_ids\"][torch.arange(outputs.logits.size(0))[:, None], last_token_positions]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15489, 128009, 128009],\n",
       "        [   575,  15489,  15489],\n",
       "        [  1288, 128009,  15489]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][torch.arange(outputs.logits.size(0))[:, None], last_token_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
