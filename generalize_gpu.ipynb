{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from prompt_processing import process_text_file\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialization of the environment\n",
    "# Load the variable from .env\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a080e4c40cc4dfba47684358a9a0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer onto GPU\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportLLMfromHF:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_embd_size(self):\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def get_nb_layers(self):\n",
    "        return self.model.config.num_hidden_layers\n",
    "\n",
    "class LayersUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, data: Dataset, method: str = \"average\"):\n",
    "        self.llm = llm\n",
    "        self.data = data\n",
    "        self.data_activation = None\n",
    "        self.group = {\"positive\": 0, \"negative\": 1}\n",
    "        self.method_fn = self.average_tokens_layers if method == \"average\" else self.final_tokens_layers if method == \"final\" else None\n",
    "        self.name = type(data).__name__\n",
    "\n",
    "        # Set tokenizer arguments based on dataset name\n",
    "        self.tokenizer_args = {\"return_tensors\": \"pt\"}\n",
    "        if self.name == \"LangLocDataset\":\n",
    "            self.tokenizer_args.update({\"truncation\": True, \"max_length\": 12})\n",
    "\n",
    "        self.extract_all_units()\n",
    "\n",
    "    def reset_data_activation(self):\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        # Move the tensor to GPU\n",
    "        self.data_activation = torch.zeros(2, len(self.data), embd_size, n_layers, device=device)\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.clear_hooks()\n",
    "        self.reset_data_activation()\n",
    "\n",
    "    def get_hook_layers(self, idx, activation):\n",
    "        def hook_layers(module, input, output):\n",
    "            activation[:, :, :, idx] = output[0].squeeze(0).to(device)\n",
    "        return hook_layers\n",
    "\n",
    "    def average_tokens_layers(self, activation):\n",
    "        return activation.mean(dim=1)\n",
    "\n",
    "    def final_tokens_layers(self, activation):\n",
    "        return activation[:, -1, :, :]\n",
    "\n",
    "    def extract_layer_units(self, idx, group_name=\"positive\", method=\"average\"):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "\n",
    "        prompt = self.data[idx][self.group[group_name]]\n",
    "        inputs = self.llm.tokenizer(prompt, **self.tokenizer_args).to(device)\n",
    "        \n",
    "        n_tokens = inputs[\"input_ids\"].shape[1]\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        \n",
    "        activation = torch.zeros(1, n_tokens, embd_size, n_layers, device=device)\n",
    "\n",
    "        for i, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_layers(i, activation))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.llm.model(**inputs)\n",
    "\n",
    "        return self.method_fn(activation)\n",
    "\n",
    "    def extract_all_units(self):\n",
    "        self.reset()\n",
    "        for idx in range(len(self.data)):\n",
    "            self.data_activation[0, idx, :, :] = self.extract_layer_units(idx, \"positive\")\n",
    "            self.data_activation[1, idx, :, :] = self.extract_layer_units(idx, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMLocDataset(Dataset):\n",
    "    \"\"\" \n",
    "    ToM Dataset for Neural Units Localization \n",
    "       \"\"\"\n",
    "    def __init__(self):\n",
    "        loc_dir = \"dataset/prompt/tom-loc\"\n",
    "        self.positive = [process_text_file(f\"{loc_dir}/{idx}b_story_question.txt\") for idx in range(1,11)]\n",
    "        self.negative = [process_text_file(f\"{loc_dir}/{idx}p_story_question.txt\") for idx in range(1,11)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "\n",
    "class LangLocDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dirpath = \"dataset/prompt/langloc\"\n",
    "        paths = glob(f\"{dirpath}/*.csv\")\n",
    "        vocab = set()\n",
    "\n",
    "        data = pd.read_csv(paths[0])\n",
    "        for path in paths[1:]:\n",
    "            run_data = pd.read_csv(path)\n",
    "            data = pd.concat([data, run_data])\n",
    "\n",
    "        data[\"sent\"] = data[\"stim2\"].apply(str.lower)\n",
    "\n",
    "        vocab.update(data[\"stim2\"].apply(str.lower).tolist())\n",
    "        for stimuli_idx in range(3, 14):\n",
    "            data[\"sent\"] += \" \" + data[f\"stim{stimuli_idx}\"].apply(str.lower)\n",
    "            vocab.update(data[f\"stim{stimuli_idx}\"].apply(str.lower).tolist())\n",
    "\n",
    "        self.vocab = sorted(list(vocab))\n",
    "        self.w2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2w = {i: w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "        self.positive = data[data[\"stim14\"]==\"S\"][\"sent\"].tolist()\n",
    "        self.negative = data[data[\"stim14\"]==\"N\"][\"sent\"].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocImportantUnits:\n",
    "    def __init__(self,\n",
    "                 checkpoint,\n",
    "                 layers_units: torch.Tensor):\n",
    "        self.model_name = checkpoint.split(\"/\")[-1]\n",
    "        self.fb_group = layers_units[0].cpu()\n",
    "        self.fp_group = layers_units[1].cpu()\n",
    "        self.t_values = self.welch_test()\n",
    "        self.ranked_units = self.get_ranked_units()\n",
    "\n",
    "    def welch_test(self):\n",
    "        n_units = self.fb_group.shape[1]\n",
    "        n_layers = self.fb_group.shape[2]\n",
    "\n",
    "        # Reshape for Welch t-test\n",
    "        fb_flattened = np.abs(self.fb_group.reshape(self.fb_group.shape[0], -1))\n",
    "        fp_flattened = np.abs(self.fp_group.reshape(self.fp_group.shape[0], -1))\n",
    "        # Perform the t-test along the first axis (sample dimension)\n",
    "        t_stat, _ = ttest_ind(fb_flattened, fp_flattened, axis=0, equal_var=False)\n",
    "        print(t_stat.shape)\n",
    "\n",
    "        # Reshape t_stat back to (units, n_layers)\n",
    "        return t_stat.reshape(n_units, n_layers)\n",
    "    \n",
    "    def get_ranked_units(self):\n",
    "        # Get ranked matrix\n",
    "        flat = self.t_values.flatten()\n",
    "        sorted_indices = np.argsort(flat)[::-1]  # Sort indices in descending order\n",
    "        ranked = np.empty_like(sorted_indices)\n",
    "        ranked[sorted_indices] = np.arange(1, len(flat) + 1)\n",
    "        # Reshape the ranked values back to the original matrix shape\n",
    "        return ranked.reshape(self.t_values.shape)\n",
    "    \n",
    "    def get_masked_ktop(self, percentage):\n",
    "        num_top_elements = int(self.t_values.size * percentage)\n",
    "        # Flatten the matrix, find the threshold value for the top 1%\n",
    "        flattened_matrix = self.t_values.flatten()\n",
    "        threshold_value = np.partition(flattened_matrix, -num_top_elements)[-num_top_elements]\n",
    "\n",
    "        # Create a binary mask where 1 represents the top 1% elements, and 0 otherwise\n",
    "        mask_units = np.where(self.t_values >= threshold_value, 1, 0)\n",
    "        return mask_units\n",
    "    \n",
    "    def get_random_mask(self, percentage, seed=None):\n",
    "        # Set the seed for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Calculate the total number of units\n",
    "        total_units = self.t_values.size\n",
    "        num_units_to_select = int(total_units * percentage)\n",
    "        \n",
    "        # Create a flattened array of zeros\n",
    "        mask_flat = np.zeros(total_units, dtype=int)\n",
    "        \n",
    "        # Randomly select indices and set them to 1\n",
    "        selected_indices = np.random.choice(total_units, num_units_to_select, replace=False)\n",
    "        mask_flat[selected_indices] = 1\n",
    "        \n",
    "        # Reshape the mask back to the original shape\n",
    "        return mask_flat.reshape(self.t_values.shape)\n",
    "    \n",
    "    def plot_layer_percentages(self, percentage, mask_type='ktop', seed=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plots the percentage of important units per layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - percentage (float): The top percentage of units to be considered as important.\n",
    "        - mask_type (str): Type of mask to use ('ktop' for k-top mask or 'random' for random mask).\n",
    "        - seed (int, optional): Random seed for reproducibility when using the random mask.\n",
    "        - save_path (str, optional): Path to save the plot. If None, shows the plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the mask based on the specified mask type\n",
    "        if mask_type == 'ktop':\n",
    "            mask = self.get_masked_ktop(percentage)\n",
    "        elif mask_type == 'random':\n",
    "            mask = self.get_random_mask(percentage, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask_type. Choose 'ktop' or 'random'.\")\n",
    "        \n",
    "        # Calculate the percentage of important units for each layer\n",
    "        layer_percentages = [(np.sum(layer) / layer.size) * 100 for layer in mask.T]\n",
    "\n",
    "        # Convert to a column vector for plotting\n",
    "        layer_percentages_matrix = np.array(layer_percentages).reshape(-1, 1)\n",
    "\n",
    "        # Plot the layer percentages as a matrix with shape (number of layers, 1)\n",
    "        plt.figure(figsize=(2, 8))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        norm = Normalize(vmin=min(layer_percentages), vmax=max(layer_percentages))\n",
    "        plt.imshow(layer_percentages_matrix, cmap=cmap, aspect='auto')\n",
    "        plt.colorbar(label=\"Percentage of Important Units\")\n",
    "        plt.title(f\"Percentage of Important Units per Layer ({mask_type.capitalize()} Mask, Top {percentage*100:.1f}%)\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "\n",
    "        # Add text annotations with adaptive color based on background brightness\n",
    "        for i, perc in enumerate(layer_percentages):\n",
    "            color = cmap(norm(perc))\n",
    "            brightness = 0.3 * color[0] + 0.5 * color[1] + 0.2 * color[2]\n",
    "            text_color = \"white\" if brightness < 0.5 else \"black\"\n",
    "            plt.text(0, i, f\"{perc:.1f}%\", ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "        # Configure ticks\n",
    "        plt.yticks(range(len(layer_percentages)), [f\"Layer {i+1}\" for i in range(len(layer_percentages))])\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Save the plot or show it based on the save_path parameter\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblateUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, mask: Optional[np.ndarray] = None):\n",
    "        self.llm = llm\n",
    "        self.mask = mask\n",
    "        self.layer_outputs = []\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = self.mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "            self.layer_outputs.append(output[0].clone().to(device))\n",
    "        return hook_ablate\n",
    "\n",
    "    def ablate_units(self, prompt):\n",
    "        self.clear_hooks()\n",
    "        self.layer_outputs.clear()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = self.llm.model.generate(**inputs, max_length=50, do_sample=True, top_p=0.95, top_k=50)\n",
    "\n",
    "        decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131072,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! What it the capital of France?\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model and localizer classes only once per session\n",
    "lang_data = LangLocDataset()\n",
    "tom_data = ToMLocDataset()\n",
    "llm = ImportLLMfromHF(model, tokenizer)\n",
    "units = LayersUnits(llm, lang_data, \"final\")\n",
    "loc_units = LocImportantUnits(checkpoint, units.data_activation)\n",
    "mask = loc_units.get_masked_ktop(0.01)\n",
    "perturbation = AblateUnits(llm, mask.T)\n",
    "perturbation.ablate_units(\"Hello! What it the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateBenchmark:\n",
    "    def __init__(self,\n",
    "                llm: ImportLLMfromHF,\n",
    "                loc_units: LocImportantUnits,\n",
    "                batch_size: int=20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "        return hook_ablate\n",
    "    \n",
    "    def get_generated_tokens(self, outputs, input_length):\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            new_tokens = output[input_length:]  # Exclude prompt tokens\n",
    "            generated_texts.append(tokenizer.decode(new_tokens, skip_special_tokens=True))\n",
    "        return generated_texts\n",
    "\n",
    "    def generate_text_with_ablations(self, prompts, mask, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "    \n",
    "    def generate_text_without_ablation(self, prompts, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "     \n",
    "    def generate(self, prompts, mask=None):\n",
    "        # Set temperature to 0 for deterministic generation\n",
    "        generation_config = {\n",
    "            \"temperature\": None,          # Explicitly unset temperature\n",
    "            \"top_p\": None,   \n",
    "            \"do_sample\": False,  # Ensures deterministic generation\n",
    "            \"pad_token_id\": tokenizer.eos_token_id  # Set the padding token if necessary\n",
    "        }\n",
    "\n",
    "        if mask is None:\n",
    "            return self.generate_text_without_ablation(prompts, **generation_config)\n",
    "        else:\n",
    "            return self.generate_text_with_ablations(prompts, mask, **generation_config)\n",
    "    \n",
    "    \n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"  \"\"\"\n",
    "        data = df.copy()\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            f\"ablate_top_{pct*100}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            f\"ablate_random1_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random2_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random3_{pct*100}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        for key, mask in assess_dict.items():\n",
    "            # Generate responses in batches and collect results\n",
    "            generated_texts = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size)):\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                generated_texts.extend(self.generate(batch_prompts, mask))\n",
    "            \n",
    "            data[f\"generate_{key}\"] = generated_texts\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "csv_file=\"dataset/benchmarks/ToMi/ToMi-finalNeuralTOM.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "# Convert the 'cands' column from string representation of lists to actual lists\n",
    "df[\"cands\"] = df[\"cands\"].apply(ast.literal_eval)\n",
    "\n",
    "intro_text = (\n",
    "    \"The following multiple choice question is based on the following story. The question \"\n",
    "    \"is related to Theory-of-Mind. Read the story and then answer the questions. Choose the best answer \"\n",
    "    \"from the options provided by printing it as is without any modifications.\"\n",
    ")\n",
    "\n",
    "df[\"prompt\"] = df.apply(\n",
    "    lambda row: f\"{intro_text}\\n\\nStory:\\n{row['story']}\\n\\nQuestion:\\n{row['question']}\\nChoose between the following options: {row['cands'][0]} or {row['cands'][1]}. The response should be contained in your first sentence.\",\n",
    "    axis=1\n",
    ")\n",
    "data = df.iloc[:20]\n",
    "assess = EvaluateBenchmark(llm, loc_units, 20)\n",
    "result_df = assess.experiment(data, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>i</th>\n",
       "      <th>dataGenSeq</th>\n",
       "      <th>sType</th>\n",
       "      <th>qTypeRaw</th>\n",
       "      <th>qTypeTomOrNot</th>\n",
       "      <th>qOrder</th>\n",
       "      <th>storyHasToM</th>\n",
       "      <th>answerMem</th>\n",
       "      <th>answerReal</th>\n",
       "      <th>qToMandOmniReader</th>\n",
       "      <th>answerMemOrReal</th>\n",
       "      <th>cands</th>\n",
       "      <th>correct</th>\n",
       "      <th>falseTrueBelief</th>\n",
       "      <th>factVsMind</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generate_no_ablation</th>\n",
       "      <th>generate_ablate_top_1.0</th>\n",
       "      <th>generate_ablate_random1_1.0</th>\n",
       "      <th>generate_ablate_random2_1.0</th>\n",
       "      <th>generate_ablate_random3_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James entered the living room. Hunter entered ...</td>\n",
       "      <td>Where is the pajamas really?</td>\n",
       "      <td>drawer</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_0,enter_agent_1,agent_0_moves_obj,...</td>\n",
       "      <td>true_belief</td>\n",
       "      <td>reality</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>False</td>\n",
       "      <td>bucket</td>\n",
       "      <td>drawer</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[bucket, drawer]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>fact</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>The pajamas are in the drawer. \\n\\nThe correc...</td>\n",
       "      <td>The  '  '  '</td>\n",
       "      <td>The pajamas are in the drawer. \\n\\nThe correc...</td>\n",
       "      <td>The correct answer is bucket. The story impli...</td>\n",
       "      <td>The pajamas are in the drawer.  The correct a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander entered the playroom. Jack entered t...</td>\n",
       "      <td>Where does Alexander think that Jack searches ...</td>\n",
       "      <td>suitcase</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_1,enter_agent_0,agent_0_moves_obj,...</td>\n",
       "      <td>true_belief</td>\n",
       "      <td>second_order_0_no_tom</td>\n",
       "      <td>False</td>\n",
       "      <td>second_order</td>\n",
       "      <td>False</td>\n",
       "      <td>box</td>\n",
       "      <td>suitcase</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[box, suitcase]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>Alexander thinks that Jack searches for the p...</td>\n",
       "      <td>'  '  '  '</td>\n",
       "      <td>\\n\\nA) box\\nB) suitcase\\nC) Both</td>\n",
       "      <td>Alexander thinks that Jack searches for the p...</td>\n",
       "      <td>Alexander thinks that Jack searches for the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jack entered the laundry. Logan entered the st...</td>\n",
       "      <td>Where will Jack look for the persimmon?</td>\n",
       "      <td>bucket</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_0,agent_2_enters,enter_agent_1,age...</td>\n",
       "      <td>second_order_false_belief</td>\n",
       "      <td>first_order_0_no_tom</td>\n",
       "      <td>False</td>\n",
       "      <td>first_order</td>\n",
       "      <td>False</td>\n",
       "      <td>pantry</td>\n",
       "      <td>bucket</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[pantry, bucket]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>Jack will look for the persimmon in the bucket.</td>\n",
       "      <td>'  '  '  '</td>\n",
       "      <td>Jack will look for the persimmon in the bucket.</td>\n",
       "      <td>Jack will look for the persimmon in the bucket.</td>\n",
       "      <td>Jack will look for the persimmon in the bucket.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  James entered the living room. Hunter entered ...   \n",
       "1  Alexander entered the playroom. Jack entered t...   \n",
       "2  Jack entered the laundry. Logan entered the st...   \n",
       "\n",
       "                                            question    answer  i  \\\n",
       "0                       Where is the pajamas really?    drawer  1   \n",
       "1  Where does Alexander think that Jack searches ...  suitcase  1   \n",
       "2            Where will Jack look for the persimmon?    bucket  1   \n",
       "\n",
       "                                          dataGenSeq  \\\n",
       "0  enter_agent_0,enter_agent_1,agent_0_moves_obj,...   \n",
       "1  enter_agent_1,enter_agent_0,agent_0_moves_obj,...   \n",
       "2  enter_agent_0,agent_2_enters,enter_agent_1,age...   \n",
       "\n",
       "                       sType               qTypeRaw  qTypeTomOrNot  \\\n",
       "0                true_belief                reality          False   \n",
       "1                true_belief  second_order_0_no_tom          False   \n",
       "2  second_order_false_belief   first_order_0_no_tom          False   \n",
       "\n",
       "         qOrder  storyHasToM answerMem answerReal  qToMandOmniReader  \\\n",
       "0       reality        False    bucket     drawer              False   \n",
       "1  second_order        False       box   suitcase              False   \n",
       "2   first_order        False    pantry     bucket              False   \n",
       "\n",
       "  answerMemOrReal             cands  correct  falseTrueBelief factVsMind  \\\n",
       "0         reality  [bucket, drawer]        1             True       fact   \n",
       "1         reality   [box, suitcase]        1             True       mind   \n",
       "2         reality  [pantry, bucket]        1             True       mind   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  The following multiple choice question is base...   \n",
       "1  The following multiple choice question is base...   \n",
       "2  The following multiple choice question is base...   \n",
       "\n",
       "                                generate_no_ablation generate_ablate_top_1.0  \\\n",
       "0   The pajamas are in the drawer. \\n\\nThe correc...            The  '  '  '   \n",
       "1   Alexander thinks that Jack searches for the p...              '  '  '  '   \n",
       "2    Jack will look for the persimmon in the bucket.              '  '  '  '   \n",
       "\n",
       "                         generate_ablate_random1_1.0  \\\n",
       "0   The pajamas are in the drawer. \\n\\nThe correc...   \n",
       "1                   \\n\\nA) box\\nB) suitcase\\nC) Both   \n",
       "2    Jack will look for the persimmon in the bucket.   \n",
       "\n",
       "                         generate_ablate_random2_1.0  \\\n",
       "0   The correct answer is bucket. The story impli...   \n",
       "1   Alexander thinks that Jack searches for the p...   \n",
       "2    Jack will look for the persimmon in the bucket.   \n",
       "\n",
       "                         generate_ablate_random3_1.0  \n",
       "0   The pajamas are in the drawer.  The correct a...  \n",
       "1   Alexander thinks that Jack searches for the p...  \n",
       "2    Jack will look for the persimmon in the bucket.  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
