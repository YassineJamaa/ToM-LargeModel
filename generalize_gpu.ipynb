{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from prompt_processing import process_text_file\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialization of the environment\n",
    "# Load the variable from .env\n",
    "pd.set_option('display.max_columns', None)\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea907517c5a84f47b0895af697d51458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer onto GPU\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportLLMfromHF:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_embd_size(self):\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def get_nb_layers(self):\n",
    "        return self.model.config.num_hidden_layers\n",
    "\n",
    "class LayersUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, data: Dataset, method: str = \"average\"):\n",
    "        self.llm = llm\n",
    "        self.data = data\n",
    "        self.data_activation = None\n",
    "        self.group = {\"positive\": 0, \"negative\": 1}\n",
    "        self.method_fn = self.average_tokens_layers if method == \"average\" else self.final_tokens_layers if method == \"final\" else None\n",
    "        self.name = type(data).__name__\n",
    "\n",
    "        # Set tokenizer arguments based on dataset name\n",
    "        self.tokenizer_args = {\"return_tensors\": \"pt\"}\n",
    "        if self.name == \"LangLocDataset\":\n",
    "            self.tokenizer_args.update({\"truncation\": True, \"max_length\": 12})\n",
    "\n",
    "        self.extract_all_units()\n",
    "\n",
    "    def reset_data_activation(self):\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        # Move the tensor to GPU\n",
    "        self.data_activation = torch.zeros(2, len(self.data), embd_size, n_layers, device=device)\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.clear_hooks()\n",
    "        self.reset_data_activation()\n",
    "\n",
    "    def get_hook_layers(self, idx, activation):\n",
    "        def hook_layers(module, input, output):\n",
    "            activation[:, :, :, idx] = output[0].squeeze(0).to(device)\n",
    "        return hook_layers\n",
    "\n",
    "    def average_tokens_layers(self, activation):\n",
    "        return activation.mean(dim=1)\n",
    "\n",
    "    def final_tokens_layers(self, activation):\n",
    "        return activation[:, -1, :, :]\n",
    "\n",
    "    def extract_layer_units(self, idx, group_name=\"positive\", method=\"average\"):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "\n",
    "        prompt = self.data[idx][self.group[group_name]]\n",
    "        inputs = self.llm.tokenizer(prompt, **self.tokenizer_args).to(device)\n",
    "        \n",
    "        n_tokens = inputs[\"input_ids\"].shape[1]\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        \n",
    "        activation = torch.zeros(1, n_tokens, embd_size, n_layers, device=device)\n",
    "\n",
    "        for i, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_layers(i, activation))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.llm.model(**inputs)\n",
    "\n",
    "        return self.method_fn(activation)\n",
    "\n",
    "    def extract_all_units(self):\n",
    "        self.reset()\n",
    "        for idx in range(len(self.data)):\n",
    "            self.data_activation[0, idx, :, :] = self.extract_layer_units(idx, \"positive\")\n",
    "            self.data_activation[1, idx, :, :] = self.extract_layer_units(idx, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMLocDataset(Dataset):\n",
    "    \"\"\" \n",
    "    ToM Dataset for Neural Units Localization \n",
    "       \"\"\"\n",
    "    def __init__(self):\n",
    "        loc_dir = \"dataset/prompt/tom-loc\"\n",
    "        self.positive = [process_text_file(f\"{loc_dir}/{idx}b_story_question.txt\") for idx in range(1,11)]\n",
    "        self.negative = [process_text_file(f\"{loc_dir}/{idx}p_story_question.txt\") for idx in range(1,11)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "\n",
    "class LangLocDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dirpath = \"dataset/prompt/langloc\"\n",
    "        paths = glob(f\"{dirpath}/*.csv\")\n",
    "        vocab = set()\n",
    "\n",
    "        data = pd.read_csv(paths[0])\n",
    "        for path in paths[1:]:\n",
    "            run_data = pd.read_csv(path)\n",
    "            data = pd.concat([data, run_data])\n",
    "\n",
    "        data[\"sent\"] = data[\"stim2\"].apply(str.lower)\n",
    "\n",
    "        vocab.update(data[\"stim2\"].apply(str.lower).tolist())\n",
    "        for stimuli_idx in range(3, 14):\n",
    "            data[\"sent\"] += \" \" + data[f\"stim{stimuli_idx}\"].apply(str.lower)\n",
    "            vocab.update(data[f\"stim{stimuli_idx}\"].apply(str.lower).tolist())\n",
    "\n",
    "        self.vocab = sorted(list(vocab))\n",
    "        self.w2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2w = {i: w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "        self.positive = data[data[\"stim14\"]==\"S\"][\"sent\"].tolist()\n",
    "        self.negative = data[data[\"stim14\"]==\"N\"][\"sent\"].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocImportantUnits:\n",
    "    def __init__(self,\n",
    "                 checkpoint,\n",
    "                 layers_units: torch.Tensor):\n",
    "        self.model_name = checkpoint.split(\"/\")[-1]\n",
    "        self.fb_group = layers_units[0].cpu()\n",
    "        self.fp_group = layers_units[1].cpu()\n",
    "        self.t_values = self.welch_test()\n",
    "        self.ranked_units = self.get_ranked_units()\n",
    "\n",
    "    def welch_test(self):\n",
    "        n_units = self.fb_group.shape[1]\n",
    "        n_layers = self.fb_group.shape[2]\n",
    "\n",
    "        # Reshape for Welch t-test\n",
    "        fb_flattened = np.abs(self.fb_group.reshape(self.fb_group.shape[0], -1))\n",
    "        fp_flattened = np.abs(self.fp_group.reshape(self.fp_group.shape[0], -1))\n",
    "        # Perform the t-test along the first axis (sample dimension)\n",
    "        t_stat, _ = ttest_ind(fb_flattened, fp_flattened, axis=0, equal_var=False)\n",
    "        print(t_stat.shape)\n",
    "\n",
    "        # Reshape t_stat back to (units, n_layers)\n",
    "        return t_stat.reshape(n_units, n_layers)\n",
    "    \n",
    "    def get_ranked_units(self):\n",
    "        # Get ranked matrix\n",
    "        flat = self.t_values.flatten()\n",
    "        sorted_indices = np.argsort(flat)[::-1]  # Sort indices in descending order\n",
    "        ranked = np.empty_like(sorted_indices)\n",
    "        ranked[sorted_indices] = np.arange(1, len(flat) + 1)\n",
    "        # Reshape the ranked values back to the original matrix shape\n",
    "        return ranked.reshape(self.t_values.shape)\n",
    "    \n",
    "    def get_masked_ktop(self, percentage):\n",
    "        num_top_elements = int(self.t_values.size * percentage)\n",
    "        # Flatten the matrix, find the threshold value for the top 1%\n",
    "        flattened_matrix = self.t_values.flatten()\n",
    "        threshold_value = np.partition(flattened_matrix, -num_top_elements)[-num_top_elements]\n",
    "\n",
    "        # Create a binary mask where 1 represents the top 1% elements, and 0 otherwise\n",
    "        mask_units = np.where(self.t_values >= threshold_value, 1, 0)\n",
    "        return mask_units\n",
    "    \n",
    "    def get_random_mask(self, percentage, seed=None):\n",
    "        # Set the seed for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Calculate the total number of units\n",
    "        total_units = self.t_values.size\n",
    "        num_units_to_select = int(total_units * percentage)\n",
    "        \n",
    "        # Create a flattened array of zeros\n",
    "        mask_flat = np.zeros(total_units, dtype=int)\n",
    "        \n",
    "        # Randomly select indices and set them to 1\n",
    "        selected_indices = np.random.choice(total_units, num_units_to_select, replace=False)\n",
    "        mask_flat[selected_indices] = 1\n",
    "        \n",
    "        # Reshape the mask back to the original shape\n",
    "        return mask_flat.reshape(self.t_values.shape)\n",
    "    \n",
    "    def plot_layer_percentages(self, percentage, mask_type='ktop', seed=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plots the percentage of important units per layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - percentage (float): The top percentage of units to be considered as important.\n",
    "        - mask_type (str): Type of mask to use ('ktop' for k-top mask or 'random' for random mask).\n",
    "        - seed (int, optional): Random seed for reproducibility when using the random mask.\n",
    "        - save_path (str, optional): Path to save the plot. If None, shows the plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the mask based on the specified mask type\n",
    "        if mask_type == 'ktop':\n",
    "            mask = self.get_masked_ktop(percentage)\n",
    "        elif mask_type == 'random':\n",
    "            mask = self.get_random_mask(percentage, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask_type. Choose 'ktop' or 'random'.\")\n",
    "        \n",
    "        # Calculate the percentage of important units for each layer\n",
    "        layer_percentages = [(np.sum(layer) / layer.size) * 100 for layer in mask.T]\n",
    "\n",
    "        # Convert to a column vector for plotting\n",
    "        layer_percentages_matrix = np.array(layer_percentages).reshape(-1, 1)\n",
    "\n",
    "        # Plot the layer percentages as a matrix with shape (number of layers, 1)\n",
    "        plt.figure(figsize=(2, 8))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        norm = Normalize(vmin=min(layer_percentages), vmax=max(layer_percentages))\n",
    "        plt.imshow(layer_percentages_matrix, cmap=cmap, aspect='auto')\n",
    "        plt.colorbar(label=\"Percentage of Important Units\")\n",
    "        plt.title(f\"Percentage of Important Units per Layer ({mask_type.capitalize()} Mask, Top {percentage*100:.1f}%)\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "\n",
    "        # Add text annotations with adaptive color based on background brightness\n",
    "        for i, perc in enumerate(layer_percentages):\n",
    "            color = cmap(norm(perc))\n",
    "            brightness = 0.3 * color[0] + 0.5 * color[1] + 0.2 * color[2]\n",
    "            text_color = \"white\" if brightness < 0.5 else \"black\"\n",
    "            plt.text(0, i, f\"{perc:.1f}%\", ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "        # Configure ticks\n",
    "        plt.yticks(range(len(layer_percentages)), [f\"Layer {i+1}\" for i in range(len(layer_percentages))])\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Save the plot or show it based on the save_path parameter\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblateUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, mask: Optional[np.ndarray] = None):\n",
    "        self.llm = llm\n",
    "        self.mask = mask\n",
    "        self.layer_outputs = []\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = self.mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "            self.layer_outputs.append(output[0].clone().to(device))\n",
    "        return hook_ablate\n",
    "\n",
    "    def ablate_units(self, prompt):\n",
    "        self.clear_hooks()\n",
    "        self.layer_outputs.clear()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = self.llm.model.generate(**inputs, max_length=50, do_sample=True, top_p=0.95, top_k=50)\n",
    "\n",
    "        decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131072,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2827654/2362297561.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647329220/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  output[0][:,:,unit_indices] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! What it the capital of France?\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model and localizer classes only once per session\n",
    "lang_data = LangLocDataset()\n",
    "tom_data = ToMLocDataset()\n",
    "llm = ImportLLMfromHF(model, tokenizer)\n",
    "units = LayersUnits(llm, lang_data, \"final\")\n",
    "loc_units = LocImportantUnits(checkpoint, units.data_activation)\n",
    "mask = loc_units.get_masked_ktop(0.01)\n",
    "perturbation = AblateUnits(llm, mask.T)\n",
    "perturbation.ablate_units(\"Hello! What it the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateBenchmark:\n",
    "    def __init__(self,\n",
    "                llm: ImportLLMfromHF,\n",
    "                loc_units: LocImportantUnits,\n",
    "                batch_size: int=20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "        return hook_ablate\n",
    "    \n",
    "    def get_generated_tokens(self, outputs, input_length):\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            new_tokens = output[input_length:]  # Exclude prompt tokens\n",
    "            generated_texts.append(tokenizer.decode(new_tokens, skip_special_tokens=True))\n",
    "        return generated_texts\n",
    "\n",
    "    def generate_text_with_ablations(self, prompts, mask, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "    \n",
    "    def generate_text_without_ablation(self, prompts, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "        inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "     \n",
    "    def generate(self, prompts, mask=None):\n",
    "        # Set temperature to 0 for deterministic generation\n",
    "        generation_config = {\n",
    "            \"temperature\": None,          # Explicitly unset temperature\n",
    "            \"top_p\": None,   \n",
    "            \"do_sample\": False,  # Ensures deterministic generation\n",
    "            \"pad_token_id\": tokenizer.eos_token_id  # Set the padding token if necessary\n",
    "        }\n",
    "\n",
    "        if mask is None:\n",
    "            return self.generate_text_without_ablation(prompts, **generation_config)\n",
    "        else:\n",
    "            return self.generate_text_with_ablations(prompts, mask, **generation_config)\n",
    "        \n",
    "    def compute_surprisal(self, prompt: str, candidates: list):\n",
    "        \"\"\"\n",
    "        Computes surprisal values for a list of candidate completions given a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The prompt for the language model.\n",
    "            candidates (list): A list of candidate completions.\n",
    "            llm (ImportLLMfromHF): Model and tokenizer wrapper.\n",
    "\n",
    "        Returns:\n",
    "            str: The candidate with the lowest surprisal score.\n",
    "        \"\"\"\n",
    "        # Determine device (GPU if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        candidates_surprisal = []\n",
    "        prompt_inputs = self.llm.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            # Tokenize candidate\n",
    "            candidate_inputs = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Concatenate prompt and candidate tokens\n",
    "            input_ids = torch.cat([prompt_inputs[\"input_ids\"], candidate_inputs[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "            # Get logits from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = llm.model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Extract logits for the candidate tokens\n",
    "            start_idx = prompt_inputs[\"input_ids\"].shape[1]\n",
    "            candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "            # Calculate negative log-probability (surprisal) for each candidate token\n",
    "            candidate_logprobs = []\n",
    "            for i, token_id in enumerate(candidate_inputs[\"input_ids\"][0, 1:]):\n",
    "                token_logits = candidate_logits[i]\n",
    "                token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                candidate_logprobs.append(-token_logprob.item())  # negative log-probability as surprisal\n",
    "\n",
    "            # Average surprisal for the candidate\n",
    "            avg_surprisal = np.mean(candidate_logprobs)\n",
    "            candidates_surprisal.append(avg_surprisal)\n",
    "\n",
    "        # Find the candidate with the lowest surprisal score\n",
    "        min_surprisal_index = np.argmin(candidates_surprisal)\n",
    "        return candidates[min_surprisal_index]\n",
    "    \n",
    "    \n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"  \"\"\"\n",
    "        data = df.copy()\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            f\"ablate_top_{pct*100}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            f\"ablate_random1_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random2_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random3_{pct*100}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        for key, mask in assess_dict.items():\n",
    "            # Generate responses in batches and collect results\n",
    "            predict_cands = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size)):\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                predict_cands.extend(self.generate(batch_prompts, mask))\n",
    "            \n",
    "            data[f\"predict_{key}\"] = predict_cands\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AssessBenchmark:\n",
    "    def __init__(self,\n",
    "                 llm: ImportLLMfromHF,\n",
    "                 loc_units: LocImportantUnits,\n",
    "                 batch_size: int = 20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Clears all registered forward hooks in the model layers.\"\"\"\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        \"\"\"\n",
    "        Defines a hook function to ablate specific units based on a mask.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Layer index.\n",
    "            mask (torch.Tensor): Binary mask for ablation at the given layer.\n",
    "\n",
    "        Returns:\n",
    "            function: A hook function to zero out specified units.\n",
    "        \"\"\"\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:, :, unit_indices] = 0\n",
    "        return hook_ablate\n",
    "\n",
    "    def surprisal(self, prompts, candidates_list, mask=None):\n",
    "        \"\"\"\n",
    "        Computes surprisal for each candidate completion in a batch of prompts and returns\n",
    "        the candidate with the lowest surprisal for each prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompts (list): List of prompt strings.\n",
    "            candidates_list (list of lists): List of candidate lists for each prompt.\n",
    "            mask (optional): Mask for ablation.\n",
    "\n",
    "        Returns:\n",
    "            list: A list where each element is the candidate with the lowest surprisal for each prompt.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        if mask is not None:\n",
    "            for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "                layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        # Tokenize prompts in a batch\n",
    "        prompt_inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        surprisal_results = []\n",
    "\n",
    "        for prompt_input, candidates in zip(prompt_inputs[\"input_ids\"], candidates_list):\n",
    "            candidate_surprisals = []\n",
    "\n",
    "            # Process each candidate individually for the current prompt\n",
    "            for candidate in candidates:\n",
    "                candidate_input = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "                # Concatenate the prompt and candidate tokens\n",
    "                input_ids = torch.cat([prompt_input.unsqueeze(0), candidate_input[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.llm.model(input_ids)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Extract logits for the candidate tokens\n",
    "                start_idx = prompt_input.shape[0]\n",
    "                candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "                # Calculate negative log-probability (surprisal) for each token in the candidate\n",
    "                candidate_logprobs = []\n",
    "                for i, token_id in enumerate(candidate_input[\"input_ids\"][0, 1:]):\n",
    "                    token_logits = candidate_logits[i]\n",
    "                    token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                    candidate_logprobs.append(-token_logprob.item())\n",
    "\n",
    "                # Calculate average surprisal for the candidate and store it\n",
    "                avg_surprisal = np.mean(candidate_logprobs)\n",
    "                candidate_surprisals.append(avg_surprisal)\n",
    "\n",
    "            # Find the index of the candidate with the lowest surprisal\n",
    "            min_index = np.argmin(candidate_surprisals)\n",
    "            surprisal_results.append(candidates[min_index])\n",
    "\n",
    "        self.clear_hooks()  # Clear hooks after computation\n",
    "        return surprisal_results\n",
    "\n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"\n",
    "        Run an experiment on a DataFrame of prompts to evaluate surprisal scores\n",
    "        with and without ablations.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame containing the prompts and candidates.\n",
    "            pct (float): The percentage of top units to ablate in some settings.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The original DataFrame with added columns for surprisal scores.\n",
    "        \"\"\"\n",
    "        data = df.copy()\n",
    "        \n",
    "        # Define different ablation masks for comparison\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            # f\"ablate_top_{int(pct * 100)}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            # f\"ablate_random1_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random2_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random3_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        # For each ablation setting, compute surprisal scores\n",
    "        for key, mask in assess_dict.items():\n",
    "            surprisal_scores = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size), desc=f\"Processing {key}\"):\n",
    "                # Fetch a batch of prompts and corresponding candidates\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                batch_candidates = data[\"cands\"].iloc[i:i+self.batch_size].tolist()\n",
    "                \n",
    "                # Calculate surprisal scores for each prompt and its list of candidates\n",
    "                batch_scores = self.surprisal(batch_prompts, batch_candidates, mask)\n",
    "                surprisal_scores.extend(batch_scores)\n",
    "            \n",
    "            # Store the surprisal scores for each ablation setting\n",
    "            data[f\"predict_{key}\"] = surprisal_scores\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing no_ablation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing no_ablation: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "csv_file=\"dataset/benchmarks/ToMi/ToMi-finalNeuralTOM.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df[df[\"qOrder\"]=='first_order'].reset_index(drop=True)\n",
    "# Convert the 'cands' column from string representation of lists to actual lists\n",
    "df[\"cands\"] = df[\"cands\"].apply(ast.literal_eval)\n",
    "\n",
    "intro_text = (\n",
    "    \"The following multiple choice question is based on the following story. The question \"\n",
    "    \"is related to Theory-of-Mind. Read the story and then answer the questions. Choose the best answer \"\n",
    "    \"from the options provided by printing it as is without any modifications.\"\n",
    ")\n",
    "\n",
    "df[\"prompt\"] = df.apply(\n",
    "    lambda row: f\"{intro_text}\\n\\nStory:\\n{row['story']}\\n\\nQuestion:\\n{row['question']}\\nChoose between the following options: {row['cands'][0]} or {row['cands'][1]}. The response should be contained in your first sentence.\",\n",
    "    axis=1\n",
    ")\n",
    "data = df.iloc[:20]\n",
    "assess = AssessBenchmark(llm, loc_units, 20)\n",
    "res = assess.experiment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_prompt': 0,\n",
       " 'id_cand': 0,\n",
       " 'cand': 'pantry',\n",
       " 'fusion': 'The following multiple choice question is based on the following story. The question is related to Theory-of-Mind. Read the story and then answer the questions. Choose the best answer from the options provided by printing it as is without any modifications.\\n\\nStory:\\nJack entered the laundry. Logan entered the study. Nathan entered the laundry. The persimmon is in the pantry. Jack moved the persimmon to the bucket. Nathan exited the laundry. Logan exited the study. Jack exited the laundry. Nathan entered the study.\\n\\nQuestion:\\nWhere will Jack look for the persimmon?\\nChoose between the following options: pantry or bucket. The response should be contained in your first sentence. pantry',\n",
       " 'prompt_end_pos': 682}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BenchmarkToMi(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.expanded_df = pd.DataFrame({\n",
    "            \"id_prompt\": self.data.index.repeat(self.data[\"cands\"].str.len()),\n",
    "            \"id_cand\": [i for sublist in self.data[\"cands\"] for i in range(len(sublist))],\n",
    "            \"cand\": [cand for cands in self.data[\"cands\"] for cand in cands],\n",
    "            \"fusion\": [f\"{prompt} {cand}\" for prompt, cands in zip(self.data[\"prompt\"], self.data[\"cands\"]) for cand in cands],\n",
    "            \"prompt_end_pos\": [\n",
    "                len(prompt) for prompt, cands in zip(self.data[\"prompt\"], self.data[\"cands\"]) for _ in cands\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.expanded_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a single data point from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the data for the specified index.\n",
    "        \"\"\"\n",
    "        row = self.expanded_df.iloc[idx]\n",
    "        return {\n",
    "            \"id_prompt\": row[\"id_prompt\"],\n",
    "            \"id_cand\": row[\"id_cand\"],\n",
    "            \"cand\": row[\"cand\"],\n",
    "            \"fusion\": row[\"fusion\"],\n",
    "            \"prompt_end_pos\": row[\"prompt_end_pos\"]\n",
    "        }\n",
    "\n",
    "class AssessBenchmark:\n",
    "    def __init__(self,\n",
    "                 bn_data: Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame (assuming data is the first 20 rows of df)\n",
    "data = df.copy()\n",
    "# Create the expanded DataFrame\n",
    "expanded_df = pd.DataFrame({\n",
    "    \"id_prompt\": data.index.repeat(data[\"cands\"].str.len()),\n",
    "    \"id_cand\": [i for sublist in data[\"cands\"] for i in range(len(sublist))],\n",
    "    \"cand\": [cand for cands in data[\"cands\"] for cand in cands],\n",
    "    \"fusion\": [f\"{prompt} {cand}\" for prompt, cands in zip(data[\"prompt\"], data[\"cands\"]) for cand in cands],\n",
    "    \"prompt_end_pos\": [\n",
    "        len(prompt) for prompt, cands in zip(data[\"prompt\"], data[\"cands\"]) for _ in cands\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Tokenize the \"fusion\" variable starting from \"prompt_end_pos\"\n",
    "expanded_df[\"last_tokens\"] = expanded_df.apply(\n",
    "    lambda row: tokenizer(row[\"fusion\"][row[\"prompt_end_pos\"]:], add_special_tokens=False)[\"input_ids\"], axis=1\n",
    ")\n",
    "max_num_tokens = expanded_df[\"last_tokens\"].apply(len).max()\n",
    "    \n",
    "batch_size = 20\n",
    "last_tokens_embeddings = []\n",
    "last_decoded_tokens = []\n",
    "all_final_positions = []\n",
    "for i in range(0, len(expanded_df), batch_size):\n",
    "    batch_texts = expanded_df[\"fusion\"].tolist()[i:i+batch_size]\n",
    "    # Tokenize the batch of texts and get input IDs\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "    last_token_positions = (inputs[\"attention_mask\"].sum(dim=1) - 1)\n",
    "\n",
    "    # Define a range of offsets for the last tokens\n",
    "    expanded_positions = last_token_positions[:, None] - torch.arange(max_num_tokens, -1, -1, device=device)\n",
    "    token_positions = torch.gather(inputs[\"input_ids\"], 1, expanded_positions)\n",
    "    # Reshape for indexing into `outputs.logits`\n",
    "    batch_indices = torch.arange(inputs[\"input_ids\"].shape[0]).unsqueeze(1).expand(-1, max_num_tokens+1)  # Shape: [20, 3]\n",
    "    batch_logits = outputs.logits[batch_indices, expanded_positions, token_positions]\n",
    "    last_tokens_embeddings.extend(batch_logits.cpu().numpy())\n",
    "    batch_decoded_tokens = [tokenizer.convert_ids_to_tokens(row.tolist()) for row in token_positions]\n",
    "    last_decoded_tokens.extend(batch_decoded_tokens)\n",
    "    all_final_positions.extend(token_positions.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_prompt</th>\n",
       "      <th>id_cand</th>\n",
       "      <th>cand</th>\n",
       "      <th>fusion</th>\n",
       "      <th>prompt_end_pos</th>\n",
       "      <th>last_tokens</th>\n",
       "      <th>last_positions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>682</td>\n",
       "      <td>[Ġsentence, ., Ġpantry]</td>\n",
       "      <td>[11914, 13, 69357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bucket</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>682</td>\n",
       "      <td>[Ġsentence, ., Ġbucket]</td>\n",
       "      <td>[11914, 13, 15994]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>605</td>\n",
       "      <td>[Ġsentence, ., Ġpantry]</td>\n",
       "      <td>[11914, 13, 69357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>drawer</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>605</td>\n",
       "      <td>[Ġsentence, ., Ġdrawer]</td>\n",
       "      <td>[11914, 13, 27580]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>bottle</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>698</td>\n",
       "      <td>[Ġsentence, ., Ġbottle]</td>\n",
       "      <td>[11914, 13, 16893]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>617</td>\n",
       "      <td>1</td>\n",
       "      <td>pantry</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>765</td>\n",
       "      <td>[Ġsentence, ., Ġpantry]</td>\n",
       "      <td>[11914, 13, 69357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>618</td>\n",
       "      <td>0</td>\n",
       "      <td>bathtub</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>661</td>\n",
       "      <td>[Ġsentence, ., Ġbathtub]</td>\n",
       "      <td>[11914, 13, 81188]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>618</td>\n",
       "      <td>1</td>\n",
       "      <td>envelope</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>661</td>\n",
       "      <td>[Ġsentence, ., Ġenvelope]</td>\n",
       "      <td>[11914, 13, 35498]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>drawer</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>627</td>\n",
       "      <td>[Ġsentence, ., Ġdrawer]</td>\n",
       "      <td>[11914, 13, 27580]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "      <td>envelope</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>627</td>\n",
       "      <td>[Ġsentence, ., Ġenvelope]</td>\n",
       "      <td>[11914, 13, 35498]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_prompt  id_cand      cand  \\\n",
       "0             0        0    pantry   \n",
       "1             0        1    bucket   \n",
       "2             1        0    pantry   \n",
       "3             1        1    drawer   \n",
       "4             2        0    bottle   \n",
       "...         ...      ...       ...   \n",
       "1235        617        1    pantry   \n",
       "1236        618        0   bathtub   \n",
       "1237        618        1  envelope   \n",
       "1238        619        0    drawer   \n",
       "1239        619        1  envelope   \n",
       "\n",
       "                                                 fusion  prompt_end_pos  \\\n",
       "0     The following multiple choice question is base...             682   \n",
       "1     The following multiple choice question is base...             682   \n",
       "2     The following multiple choice question is base...             605   \n",
       "3     The following multiple choice question is base...             605   \n",
       "4     The following multiple choice question is base...             698   \n",
       "...                                                 ...             ...   \n",
       "1235  The following multiple choice question is base...             765   \n",
       "1236  The following multiple choice question is base...             661   \n",
       "1237  The following multiple choice question is base...             661   \n",
       "1238  The following multiple choice question is base...             627   \n",
       "1239  The following multiple choice question is base...             627   \n",
       "\n",
       "                    last_tokens      last_positions  \n",
       "0       [Ġsentence, ., Ġpantry]  [11914, 13, 69357]  \n",
       "1       [Ġsentence, ., Ġbucket]  [11914, 13, 15994]  \n",
       "2       [Ġsentence, ., Ġpantry]  [11914, 13, 69357]  \n",
       "3       [Ġsentence, ., Ġdrawer]  [11914, 13, 27580]  \n",
       "4       [Ġsentence, ., Ġbottle]  [11914, 13, 16893]  \n",
       "...                         ...                 ...  \n",
       "1235    [Ġsentence, ., Ġpantry]  [11914, 13, 69357]  \n",
       "1236   [Ġsentence, ., Ġbathtub]  [11914, 13, 81188]  \n",
       "1237  [Ġsentence, ., Ġenvelope]  [11914, 13, 35498]  \n",
       "1238    [Ġsentence, ., Ġdrawer]  [11914, 13, 27580]  \n",
       "1239  [Ġsentence, ., Ġenvelope]  [11914, 13, 35498]  \n",
       "\n",
       "[1240 rows x 7 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df[\"last_tokens\"] = last_decoded_tokens\n",
    "expanded_df[\"last_positions\"] = all_final_positions\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
