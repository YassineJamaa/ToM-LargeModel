{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from prompt_processing import process_text_file\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialization of the environment\n",
    "# Load the variable from .env\n",
    "pd.set_option('display.max_columns', None)\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1acc29751b242e9a87ec041a1d4e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer onto GPU\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir, token=hf_access_token).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportLLMfromHF:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def get_embd_size(self):\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def get_nb_layers(self):\n",
    "        return self.model.config.num_hidden_layers\n",
    "\n",
    "class LayersUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, data: Dataset, method: str = \"average\"):\n",
    "        self.llm = llm\n",
    "        self.data = data\n",
    "        self.data_activation = None\n",
    "        self.group = {\"positive\": 0, \"negative\": 1}\n",
    "        self.method_fn = self.average_tokens_layers if method == \"average\" else self.final_tokens_layers if method == \"final\" else None\n",
    "        self.name = type(data).__name__\n",
    "\n",
    "        # Set tokenizer arguments based on dataset name\n",
    "        self.tokenizer_args = {\"return_tensors\": \"pt\"}\n",
    "        if self.name == \"LangLocDataset\":\n",
    "            self.tokenizer_args.update({\"truncation\": True, \"max_length\": 12})\n",
    "\n",
    "        self.extract_all_units()\n",
    "\n",
    "    def reset_data_activation(self):\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        # Move the tensor to GPU\n",
    "        self.data_activation = torch.zeros(2, len(self.data), embd_size, n_layers, device=device)\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def reset(self):\n",
    "        self.clear_hooks()\n",
    "        self.reset_data_activation()\n",
    "\n",
    "    def get_hook_layers(self, idx, activation):\n",
    "        def hook_layers(module, input, output):\n",
    "            activation[:, :, :, idx] = output[0].squeeze(0).to(device)\n",
    "        return hook_layers\n",
    "\n",
    "    def average_tokens_layers(self, activation):\n",
    "        return activation.mean(dim=1)\n",
    "\n",
    "    def final_tokens_layers(self, activation):\n",
    "        return activation[:, -1, :, :]\n",
    "\n",
    "    def extract_layer_units(self, idx, group_name=\"positive\", method=\"average\"):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "\n",
    "        prompt = self.data[idx][self.group[group_name]]\n",
    "        inputs = self.llm.tokenizer(prompt, **self.tokenizer_args).to(device)\n",
    "        \n",
    "        n_tokens = inputs[\"input_ids\"].shape[1]\n",
    "        embd_size = self.llm.get_embd_size()\n",
    "        n_layers = self.llm.get_nb_layers()\n",
    "        \n",
    "        activation = torch.zeros(1, n_tokens, embd_size, n_layers, device=device)\n",
    "\n",
    "        for i, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_layers(i, activation))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.llm.model(**inputs)\n",
    "\n",
    "        return self.method_fn(activation)\n",
    "\n",
    "    def extract_all_units(self):\n",
    "        self.reset()\n",
    "        for idx in range(len(self.data)):\n",
    "            self.data_activation[0, idx, :, :] = self.extract_layer_units(idx, \"positive\")\n",
    "            self.data_activation[1, idx, :, :] = self.extract_layer_units(idx, \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_options(question):\n",
    "    options = [\"True\", \"False\"]\n",
    "    return f\"{question}\\nOptions:\\n- {options[0]}\\n- {options[1]}\"\n",
    "\n",
    "def read_story(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        story = [line.strip() for line in f.readlines()]\n",
    "    return ' '.join(story)\n",
    "\n",
    "def read_question(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        question = f.read().split('\\n\\n')[0]\n",
    "    return append_options(question.replace(\"\\n\", \"\"))\n",
    "\n",
    "class ToMLocDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        instruction = \"In this experiment, you will read a series of sentences and then answer True/False questions about them. Press button 1 to answer 'true' and button 2 to answer 'false'.\"\n",
    "        context_template = \"{instruction}\\nStory: {story}\\nQuestion: {question}\\nAnswer: {answer}\"\n",
    "        belief_stories = [read_story(f\"dataset/prompt/tomloc/{idx}b_story.txt\") for idx in range(1, 11)]\n",
    "        photograph_stories = [read_story(f\"dataset/prompt/tomloc/{idx}p_story.txt\") for idx in range(1, 11)]\n",
    "\n",
    "        belief_question = [read_question(f\"dataset/prompt/tomloc/{idx}b_question.txt\") for idx in range(1, 11)]\n",
    "        photograph_question = [read_question(f\"dataset/prompt/tomloc/{idx}p_question.txt\") for idx in range(1, 11)]\n",
    "\n",
    "        self.positive = [context_template.format(instruction=instruction, story=story, question=question, answer=np.random.choice([\"True\", \"False\"])) for story, question in zip(belief_stories, belief_question)]\n",
    "        self.negative = [context_template.format(instruction=instruction, story=story, question=question, answer=np.random.choice([\"True\", \"False\"])) for story, question in zip(photograph_stories, photograph_question)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.positive)\n",
    "\n",
    "class LangLocDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dirpath = \"dataset/prompt/langloc\"\n",
    "        paths = glob(f\"{dirpath}/*.csv\")\n",
    "        vocab = set()\n",
    "\n",
    "        data = pd.read_csv(paths[0])\n",
    "        for path in paths[1:]:\n",
    "            run_data = pd.read_csv(path)\n",
    "            data = pd.concat([data, run_data])\n",
    "\n",
    "        data[\"sent\"] = data[\"stim2\"].apply(str.lower)\n",
    "\n",
    "        vocab.update(data[\"stim2\"].apply(str.lower).tolist())\n",
    "        for stimuli_idx in range(3, 14):\n",
    "            data[\"sent\"] += \" \" + data[f\"stim{stimuli_idx}\"].apply(str.lower)\n",
    "            vocab.update(data[f\"stim{stimuli_idx}\"].apply(str.lower).tolist())\n",
    "\n",
    "        self.vocab = sorted(list(vocab))\n",
    "        self.w2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2w = {i: w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "        self.positive = data[data[\"stim14\"]==\"S\"][\"sent\"].tolist()\n",
    "        self.negative = data[data[\"stim14\"]==\"N\"][\"sent\"].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.positive[idx].strip(), self.negative[idx].strip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocImportantUnits:\n",
    "    def __init__(self,\n",
    "                 checkpoint,\n",
    "                 layers_units: torch.Tensor):\n",
    "        self.model_name = checkpoint.split(\"/\")[-1]\n",
    "        self.fb_group = layers_units[0].cpu()\n",
    "        self.fp_group = layers_units[1].cpu()\n",
    "        self.t_values = self.welch_test()\n",
    "        self.ranked_units = self.get_ranked_units()\n",
    "\n",
    "    def welch_test(self):\n",
    "        n_units = self.fb_group.shape[1]\n",
    "        n_layers = self.fb_group.shape[2]\n",
    "\n",
    "        # Reshape for Welch t-test\n",
    "        fb_flattened = np.abs(self.fb_group.reshape(self.fb_group.shape[0], -1))\n",
    "        fp_flattened = np.abs(self.fp_group.reshape(self.fp_group.shape[0], -1))\n",
    "        # Perform the t-test along the first axis (sample dimension)\n",
    "        t_stat, _ = ttest_ind(fb_flattened, fp_flattened, axis=0, equal_var=False)\n",
    "        print(t_stat.shape)\n",
    "\n",
    "        # Reshape t_stat back to (units, n_layers)\n",
    "        return t_stat.reshape(n_units, n_layers)\n",
    "    \n",
    "    def get_ranked_units(self):\n",
    "        # Get ranked matrix\n",
    "        flat = self.t_values.flatten()\n",
    "        sorted_indices = np.argsort(flat)[::-1]  # Sort indices in descending order\n",
    "        ranked = np.empty_like(sorted_indices)\n",
    "        ranked[sorted_indices] = np.arange(1, len(flat) + 1)\n",
    "        # Reshape the ranked values back to the original matrix shape\n",
    "        return ranked.reshape(self.t_values.shape)\n",
    "    \n",
    "    def get_masked_ktop(self, percentage):\n",
    "        num_top_elements = int(self.t_values.size * percentage)\n",
    "        # Flatten the matrix, find the threshold value for the top 1%\n",
    "        flattened_matrix = self.t_values.flatten()\n",
    "        threshold_value = np.partition(flattened_matrix, -num_top_elements)[-num_top_elements]\n",
    "\n",
    "        # Create a binary mask where 1 represents the top 1% elements, and 0 otherwise\n",
    "        mask_units = np.where(self.t_values >= threshold_value, 1, 0)\n",
    "        return mask_units\n",
    "    \n",
    "    def get_random_mask(self, percentage, seed=None):\n",
    "        # Set the seed for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Calculate the total number of units\n",
    "        total_units = self.t_values.size\n",
    "        num_units_to_select = int(total_units * percentage)\n",
    "        \n",
    "        # Create a flattened array of zeros\n",
    "        mask_flat = np.zeros(total_units, dtype=int)\n",
    "        \n",
    "        # Randomly select indices and set them to 1\n",
    "        selected_indices = np.random.choice(total_units, num_units_to_select, replace=False)\n",
    "        mask_flat[selected_indices] = 1\n",
    "        \n",
    "        # Reshape the mask back to the original shape\n",
    "        return mask_flat.reshape(self.t_values.shape)\n",
    "    \n",
    "    def plot_layer_percentages(self, percentage, mask_type='ktop', seed=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plots the percentage of important units per layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - percentage (float): The top percentage of units to be considered as important.\n",
    "        - mask_type (str): Type of mask to use ('ktop' for k-top mask or 'random' for random mask).\n",
    "        - seed (int, optional): Random seed for reproducibility when using the random mask.\n",
    "        - save_path (str, optional): Path to save the plot. If None, shows the plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the mask based on the specified mask type\n",
    "        if mask_type == 'ktop':\n",
    "            mask = self.get_masked_ktop(percentage)\n",
    "        elif mask_type == 'random':\n",
    "            mask = self.get_random_mask(percentage, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask_type. Choose 'ktop' or 'random'.\")\n",
    "        \n",
    "        # Calculate the percentage of important units for each layer\n",
    "        layer_percentages = [(np.sum(layer) / layer.size) * 100 for layer in mask.T]\n",
    "\n",
    "        # Convert to a column vector for plotting\n",
    "        layer_percentages_matrix = np.array(layer_percentages).reshape(-1, 1)\n",
    "\n",
    "        # Plot the layer percentages as a matrix with shape (number of layers, 1)\n",
    "        plt.figure(figsize=(2, 8))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        norm = Normalize(vmin=min(layer_percentages), vmax=max(layer_percentages))\n",
    "        plt.imshow(layer_percentages_matrix, cmap=cmap, aspect='auto')\n",
    "        plt.colorbar(label=\"Percentage of Important Units\")\n",
    "        plt.title(f\"Percentage of Important Units per Layer ({mask_type.capitalize()} Mask, Top {percentage*100:.1f}%)\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "\n",
    "        # Add text annotations with adaptive color based on background brightness\n",
    "        for i, perc in enumerate(layer_percentages):\n",
    "            color = cmap(norm(perc))\n",
    "            brightness = 0.3 * color[0] + 0.5 * color[1] + 0.2 * color[2]\n",
    "            text_color = \"white\" if brightness < 0.5 else \"black\"\n",
    "            plt.text(0, i, f\"{perc:.1f}%\", ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "        # Configure ticks\n",
    "        plt.yticks(range(len(layer_percentages)), [f\"Layer {i+1}\" for i in range(len(layer_percentages))])\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Save the plot or show it based on the save_path parameter\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblateUnits:\n",
    "    def __init__(self, llm: ImportLLMfromHF, mask: Optional[np.ndarray] = None):\n",
    "        self.llm = llm\n",
    "        self.mask = mask\n",
    "        self.layer_outputs = []\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = self.mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "            self.layer_outputs.append(output[0].clone().to(device))\n",
    "        return hook_ablate\n",
    "\n",
    "    def ablate_units(self, prompt):\n",
    "        self.clear_hooks()\n",
    "        self.layer_outputs.clear()\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = self.llm.model.generate(**inputs, max_length=50, do_sample=True, top_p=0.95, top_k=50)\n",
    "\n",
    "        decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131072,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and localizer classes only once per session\n",
    "lang_data = LangLocDataset()\n",
    "tom_data = ToMLocDataset()\n",
    "llm = ImportLLMfromHF(model, tokenizer)\n",
    "units = LayersUnits(llm, tom_data)\n",
    "loc_units = LocImportantUnits(checkpoint, units.data_activation)\n",
    "mask = loc_units.get_masked_ktop(0.01)\n",
    "# perturbation = AblateUnits(llm, mask.T)\n",
    "# perturbation.ablate_units(\"Hello! What it the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateBenchmark:\n",
    "    def __init__(self,\n",
    "                llm: ImportLLMfromHF,\n",
    "                loc_units: LocImportantUnits,\n",
    "                batch_size: int=20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "\n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:,:,unit_indices] = 0\n",
    "        return hook_ablate\n",
    "    \n",
    "    def get_generated_tokens(self, outputs, input_length):\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            new_tokens = output[input_length:]  # Exclude prompt tokens\n",
    "            generated_texts.append(tokenizer.decode(new_tokens, skip_special_tokens=True))\n",
    "        return generated_texts\n",
    "\n",
    "    def generate_text_with_ablations(self, prompts, mask, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "            layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "    \n",
    "    def generate_text_without_ablation(self, prompts, **generate_kwargs):\n",
    "        self.clear_hooks()\n",
    "        self.llm.model.eval()\n",
    "        inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", truncation=True).to(self.llm.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]  # Get the length of the prompt tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm.model.generate(**inputs, max_new_tokens=12, **generate_kwargs)\n",
    "\n",
    "        # Slice generated tokens to exclude the initial prompt tokens\n",
    "        return self.get_generated_tokens(outputs, input_length)\n",
    "     \n",
    "    def generate(self, prompts, mask=None):\n",
    "        # Set temperature to 0 for deterministic generation\n",
    "        generation_config = {\n",
    "            \"temperature\": None,          # Explicitly unset temperature\n",
    "            \"top_p\": None,   \n",
    "            \"do_sample\": False,  # Ensures deterministic generation\n",
    "            \"pad_token_id\": tokenizer.eos_token_id  # Set the padding token if necessary\n",
    "        }\n",
    "\n",
    "        if mask is None:\n",
    "            return self.generate_text_without_ablation(prompts, **generation_config)\n",
    "        else:\n",
    "            return self.generate_text_with_ablations(prompts, mask, **generation_config)\n",
    "        \n",
    "    def compute_surprisal(self, prompt: str, candidates: list):\n",
    "        \"\"\"\n",
    "        Computes surprisal values for a list of candidate completions given a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The prompt for the language model.\n",
    "            candidates (list): A list of candidate completions.\n",
    "            llm (ImportLLMfromHF): Model and tokenizer wrapper.\n",
    "\n",
    "        Returns:\n",
    "            str: The candidate with the lowest surprisal score.\n",
    "        \"\"\"\n",
    "        # Determine device (GPU if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        candidates_surprisal = []\n",
    "        prompt_inputs = self.llm.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            # Tokenize candidate\n",
    "            candidate_inputs = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Concatenate prompt and candidate tokens\n",
    "            input_ids = torch.cat([prompt_inputs[\"input_ids\"], candidate_inputs[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "            # Get logits from the model\n",
    "            with torch.no_grad():\n",
    "                outputs = llm.model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Extract logits for the candidate tokens\n",
    "            start_idx = prompt_inputs[\"input_ids\"].shape[1]\n",
    "            candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "            # Calculate negative log-probability (surprisal) for each candidate token\n",
    "            candidate_logprobs = []\n",
    "            for i, token_id in enumerate(candidate_inputs[\"input_ids\"][0, 1:]):\n",
    "                token_logits = candidate_logits[i]\n",
    "                token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                candidate_logprobs.append(-token_logprob.item())  # negative log-probability as surprisal\n",
    "\n",
    "            # Average surprisal for the candidate\n",
    "            avg_surprisal = np.mean(candidate_logprobs)\n",
    "            candidates_surprisal.append(avg_surprisal)\n",
    "\n",
    "        # Find the candidate with the lowest surprisal score\n",
    "        min_surprisal_index = np.argmin(candidates_surprisal)\n",
    "        return candidates[min_surprisal_index]\n",
    "    \n",
    "    \n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"  \"\"\"\n",
    "        data = df.copy()\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            f\"ablate_top_{pct*100}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            f\"ablate_random1_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random2_{pct*100}\": self.loc_units.get_random_mask(pct).T,\n",
    "            f\"ablate_random3_{pct*100}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        for key, mask in assess_dict.items():\n",
    "            # Generate responses in batches and collect results\n",
    "            predict_cands = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size)):\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                predict_cands.extend(self.generate(batch_prompts, mask))\n",
    "            \n",
    "            data[f\"predict_{key}\"] = predict_cands\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AssessBenchmark:\n",
    "    def __init__(self,\n",
    "                 llm: ImportLLMfromHF,\n",
    "                 loc_units: LocImportantUnits,\n",
    "                 batch_size: int = 20):\n",
    "        self.llm = llm\n",
    "        self.loc_units = loc_units\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Clears all registered forward hooks in the model layers.\"\"\"\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        \"\"\"\n",
    "        Defines a hook function to ablate specific units based on a mask.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Layer index.\n",
    "            mask (torch.Tensor): Binary mask for ablation at the given layer.\n",
    "\n",
    "        Returns:\n",
    "            function: A hook function to zero out specified units.\n",
    "        \"\"\"\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:, :, unit_indices] = 0\n",
    "        return hook_ablate\n",
    "\n",
    "    def surprisal(self, prompts, candidates_list, mask=None):\n",
    "        \"\"\"\n",
    "        Computes surprisal for each candidate completion in a batch of prompts and returns\n",
    "        the candidate with the lowest surprisal for each prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompts (list): List of prompt strings.\n",
    "            candidates_list (list of lists): List of candidate lists for each prompt.\n",
    "            mask (optional): Mask for ablation.\n",
    "\n",
    "        Returns:\n",
    "            list: A list where each element is the candidate with the lowest surprisal for each prompt.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "        self.clear_hooks()\n",
    "        \n",
    "        if mask is not None:\n",
    "            for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "                layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        # Tokenize prompts in a batch\n",
    "        prompt_inputs = self.llm.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        surprisal_results = []\n",
    "\n",
    "        for prompt_input, candidates in zip(prompt_inputs[\"input_ids\"], candidates_list):\n",
    "            candidate_surprisals = []\n",
    "\n",
    "            # Process each candidate individually for the current prompt\n",
    "            for candidate in candidates:\n",
    "                candidate_input = self.llm.tokenizer(candidate, return_tensors=\"pt\").to(device)\n",
    "                # Concatenate the prompt and candidate tokens\n",
    "                input_ids = torch.cat([prompt_input.unsqueeze(0), candidate_input[\"input_ids\"][:, 1:]], dim=-1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.llm.model(input_ids)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Extract logits for the candidate tokens\n",
    "                start_idx = prompt_input.shape[0]\n",
    "                candidate_logits = logits[0, start_idx:, :]\n",
    "\n",
    "                # Calculate negative log-probability (surprisal) for each token in the candidate\n",
    "                candidate_logprobs = []\n",
    "                for i, token_id in enumerate(candidate_input[\"input_ids\"][0, 1:]):\n",
    "                    token_logits = candidate_logits[i]\n",
    "                    token_logprob = torch.log_softmax(token_logits, dim=-1)[token_id]\n",
    "                    candidate_logprobs.append(-token_logprob.item())\n",
    "\n",
    "                # Calculate average surprisal for the candidate and store it\n",
    "                avg_surprisal = np.mean(candidate_logprobs)\n",
    "                candidate_surprisals.append(avg_surprisal)\n",
    "\n",
    "            # Find the index of the candidate with the lowest surprisal\n",
    "            min_index = np.argmin(candidate_surprisals)\n",
    "            surprisal_results.append(candidates[min_index])\n",
    "\n",
    "        self.clear_hooks()  # Clear hooks after computation\n",
    "        return surprisal_results\n",
    "\n",
    "    def experiment(self, df, pct=0.01):\n",
    "        \"\"\"\n",
    "        Run an experiment on a DataFrame of prompts to evaluate surprisal scores\n",
    "        with and without ablations.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame containing the prompts and candidates.\n",
    "            pct (float): The percentage of top units to ablate in some settings.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The original DataFrame with added columns for surprisal scores.\n",
    "        \"\"\"\n",
    "        data = df.copy()\n",
    "        \n",
    "        # Define different ablation masks for comparison\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            # f\"ablate_top_{int(pct * 100)}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            # f\"ablate_random1_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random2_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random3_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "\n",
    "        # For each ablation setting, compute surprisal scores\n",
    "        for key, mask in assess_dict.items():\n",
    "            surprisal_scores = []\n",
    "            for i in tqdm(range(0, len(data), self.batch_size), desc=f\"Processing {key}\"):\n",
    "                # Fetch a batch of prompts and corresponding candidates\n",
    "                batch_prompts = data[\"prompt\"].iloc[i:i+self.batch_size].tolist()\n",
    "                batch_candidates = data[\"cands\"].iloc[i:i+self.batch_size].tolist()\n",
    "                \n",
    "                # Calculate surprisal scores for each prompt and its list of candidates\n",
    "                batch_scores = self.surprisal(batch_prompts, batch_candidates, mask)\n",
    "                surprisal_scores.extend(batch_scores)\n",
    "            \n",
    "            # Store the surprisal scores for each ablation setting\n",
    "            data[f\"predict_{key}\"] = surprisal_scores\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n",
      "100%|██████████| 62/62 [00:55<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>story</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>i</th>\n",
       "      <th>dataGenSeq</th>\n",
       "      <th>sType</th>\n",
       "      <th>qTypeRaw</th>\n",
       "      <th>qTypeTomOrNot</th>\n",
       "      <th>qOrder</th>\n",
       "      <th>storyHasToM</th>\n",
       "      <th>answerMem</th>\n",
       "      <th>answerReal</th>\n",
       "      <th>qToMandOmniReader</th>\n",
       "      <th>answerMemOrReal</th>\n",
       "      <th>cands</th>\n",
       "      <th>correct</th>\n",
       "      <th>falseTrueBelief</th>\n",
       "      <th>factVsMind</th>\n",
       "      <th>prompt</th>\n",
       "      <th>predict_no_ablation</th>\n",
       "      <th>predict_ablate_top_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jack entered the laundry. Logan entered the st...</td>\n",
       "      <td>Where will Jack look for the persimmon?</td>\n",
       "      <td>bucket</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_0,agent_2_enters,enter_agent_1,age...</td>\n",
       "      <td>second_order_false_belief</td>\n",
       "      <td>first_order_0_no_tom</td>\n",
       "      <td>False</td>\n",
       "      <td>first_order</td>\n",
       "      <td>False</td>\n",
       "      <td>pantry</td>\n",
       "      <td>bucket</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[pantry, bucket]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>bucket</td>\n",
       "      <td>bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ella entered the dining room. Carter entered t...</td>\n",
       "      <td>Where will Ella look for the lemon?</td>\n",
       "      <td>drawer</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_1,enter_agent_0,agent_1_exits,agen...</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>first_order_0_tom</td>\n",
       "      <td>True</td>\n",
       "      <td>first_order</td>\n",
       "      <td>True</td>\n",
       "      <td>drawer</td>\n",
       "      <td>pantry</td>\n",
       "      <td>False</td>\n",
       "      <td>memory</td>\n",
       "      <td>[pantry, drawer]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>drawer</td>\n",
       "      <td>drawer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mason entered the bedroom. Isabella entered th...</td>\n",
       "      <td>Where will Mason look for the pineapple?</td>\n",
       "      <td>crate</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_0,enter_agent_1,agent_2_enters,age...</td>\n",
       "      <td>second_order_false_belief</td>\n",
       "      <td>first_order_0_no_tom</td>\n",
       "      <td>False</td>\n",
       "      <td>first_order</td>\n",
       "      <td>False</td>\n",
       "      <td>bottle</td>\n",
       "      <td>crate</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[bottle, crate]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>crate</td>\n",
       "      <td>crate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mason entered the sunroom. Ella entered the su...</td>\n",
       "      <td>Where will Mason look for the lemon?</td>\n",
       "      <td>bottle</td>\n",
       "      <td>1</td>\n",
       "      <td>enter_agent_0,enter_agent_1,agent_1_exits,agen...</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>first_order_0_no_tom</td>\n",
       "      <td>False</td>\n",
       "      <td>first_order</td>\n",
       "      <td>False</td>\n",
       "      <td>bathtub</td>\n",
       "      <td>bottle</td>\n",
       "      <td>False</td>\n",
       "      <td>reality</td>\n",
       "      <td>[bottle, bathtub]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>mind</td>\n",
       "      <td>The following multiple choice question is base...</td>\n",
       "      <td>bathtub</td>\n",
       "      <td>bathtub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              story  \\\n",
       "0      0  Jack entered the laundry. Logan entered the st...   \n",
       "1      1  Ella entered the dining room. Carter entered t...   \n",
       "2      2  Mason entered the bedroom. Isabella entered th...   \n",
       "3      3  Mason entered the sunroom. Ella entered the su...   \n",
       "\n",
       "                                   question  answer  i  \\\n",
       "0   Where will Jack look for the persimmon?  bucket  1   \n",
       "1       Where will Ella look for the lemon?  drawer  1   \n",
       "2  Where will Mason look for the pineapple?   crate  1   \n",
       "3      Where will Mason look for the lemon?  bottle  1   \n",
       "\n",
       "                                          dataGenSeq  \\\n",
       "0  enter_agent_0,agent_2_enters,enter_agent_1,age...   \n",
       "1  enter_agent_1,enter_agent_0,agent_1_exits,agen...   \n",
       "2  enter_agent_0,enter_agent_1,agent_2_enters,age...   \n",
       "3  enter_agent_0,enter_agent_1,agent_1_exits,agen...   \n",
       "\n",
       "                       sType              qTypeRaw  qTypeTomOrNot  \\\n",
       "0  second_order_false_belief  first_order_0_no_tom          False   \n",
       "1               false_belief     first_order_0_tom           True   \n",
       "2  second_order_false_belief  first_order_0_no_tom          False   \n",
       "3               false_belief  first_order_0_no_tom          False   \n",
       "\n",
       "        qOrder  storyHasToM answerMem answerReal  qToMandOmniReader  \\\n",
       "0  first_order        False    pantry     bucket              False   \n",
       "1  first_order         True    drawer     pantry              False   \n",
       "2  first_order        False    bottle      crate              False   \n",
       "3  first_order        False   bathtub     bottle              False   \n",
       "\n",
       "  answerMemOrReal              cands  correct  falseTrueBelief factVsMind  \\\n",
       "0         reality   [pantry, bucket]        1             True       mind   \n",
       "1          memory   [pantry, drawer]        1            False       mind   \n",
       "2         reality    [bottle, crate]        1             True       mind   \n",
       "3         reality  [bottle, bathtub]        0             True       mind   \n",
       "\n",
       "                                              prompt predict_no_ablation  \\\n",
       "0  The following multiple choice question is base...              bucket   \n",
       "1  The following multiple choice question is base...              drawer   \n",
       "2  The following multiple choice question is base...               crate   \n",
       "3  The following multiple choice question is base...             bathtub   \n",
       "\n",
       "  predict_ablate_top_1  \n",
       "0               bucket  \n",
       "1               drawer  \n",
       "2                crate  \n",
       "3              bathtub  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "class BenchmarkToMi(Dataset):\n",
    "    def __init__(self):\n",
    "        csv_file=\"dataset/benchmarks/ToMi/ToMi-finalNeuralTOM.csv\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df[df[\"qOrder\"]=='first_order'].reset_index(drop=True)\n",
    "        # Convert the 'cands' column from string representation of lists to actual lists\n",
    "        df[\"cands\"] = df[\"cands\"].apply(ast.literal_eval)\n",
    "        context = (\n",
    "            \"The following multiple choice question is based on the following story. The question \"\n",
    "            \"is related to Theory-of-Mind. Read the story and then answer the questions. Choose the best answer \"\n",
    "            \"from the options provided by printing it as is without any modifications.\"\n",
    "        )\n",
    "        instruction = \"In this experiment, you will read a series of sentences and then answer True/False questions about them. Press button 1 to answer 'true' and button 2 to answer 'false'.\"\n",
    "\n",
    "        df[\"prompt\"] = df.apply(\n",
    "            lambda row: f\"{context}\\nStory: {row[\"story\"]}\\nQuestion: {row[\"question\"]}\\nOptions:\\n- {row[\"cands\"][0]}\\n- {row[\"cands\"][1]}\\nAnswer:\",\n",
    "            axis=1\n",
    "        )\n",
    "        # df = df.iloc[:20]\n",
    "        self.data = df\n",
    "        self.expanded_df = pd.DataFrame({\n",
    "            \"id_prompt\": self.data.index.repeat(self.data[\"cands\"].str.len()),\n",
    "            \"id_cand\": [i for sublist in self.data[\"cands\"] for i in range(len(sublist))],\n",
    "            \"cand\": [cand for cands in self.data[\"cands\"] for cand in cands],\n",
    "            \"fusion\": [f\"{prompt} {cand}\" for prompt, cands in zip(self.data[\"prompt\"], self.data[\"cands\"]) for cand in cands],\n",
    "            \"prompt_end_pos\": [\n",
    "                len(prompt) for prompt, cands in zip(self.data[\"prompt\"], self.data[\"cands\"]) for _ in cands\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Tokenize the \"fusion\" variable starting from \"prompt_end_pos\"\n",
    "        self.expanded_df[\"last_tokens\"] = self.expanded_df.apply(\n",
    "            lambda row: tokenizer(row[\"fusion\"][row[\"prompt_end_pos\"]:], add_special_tokens=False)[\"input_ids\"], axis=1\n",
    "        ).apply(len)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.expanded_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a single data point from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the data for the specified index.\n",
    "        \"\"\"\n",
    "        row = self.expanded_df.iloc[idx]\n",
    "        return {\n",
    "            \"id_prompt\": row[\"id_prompt\"],\n",
    "            \"id_cand\": row[\"id_cand\"],\n",
    "            \"cand\": row[\"cand\"],\n",
    "            \"fusion\": row[\"fusion\"],\n",
    "            \"prompt_end_pos\": row[\"prompt_end_pos\"]\n",
    "        }\n",
    "\n",
    "def compute_cand_score(row):\n",
    "        logits = row['last_embds']\n",
    "        n = row['last_tokens']\n",
    "        if n == 1:\n",
    "            return logits[-1]  # Return the last element\n",
    "        elif n > 1:\n",
    "            return sum(logits[-n:]) / n  # Return the mean of the last N elements\n",
    "\n",
    "class AssessBenchmark:\n",
    "    def __init__(self,\n",
    "                 llm: ImportLLMfromHF,\n",
    "                 loc_units: LocImportantUnits,\n",
    "                 batch_size: int = 20):\n",
    "        self.llm = llm\n",
    "        self.batch_size = batch_size\n",
    "        self.loc_units = loc_units\n",
    "\n",
    "        if self.llm.tokenizer.pad_token is None:\n",
    "            self.llm.tokenizer.pad_token = self.llm.tokenizer.eos_token\n",
    "    \n",
    "    def clear_hooks(self):\n",
    "        \"\"\"Clears all registered forward hooks in the model layers.\"\"\"\n",
    "        for layer in self.llm.model.model.layers:\n",
    "            layer._forward_hooks.clear()\n",
    "    \n",
    "    def get_hook_ablate(self, idx, mask):\n",
    "        \"\"\"\n",
    "        Defines a hook function to ablate specific units based on a mask.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Layer index.\n",
    "            mask (torch.Tensor): Binary mask for ablation at the given layer.\n",
    "\n",
    "        Returns:\n",
    "            function: A hook function to zero out specified units.\n",
    "        \"\"\"\n",
    "        def hook_ablate(module, input, output):\n",
    "            mask_layer = mask[idx]\n",
    "            unit_indices = mask_layer.nonzero()\n",
    "            output[0][:, :, unit_indices] = 0\n",
    "        return hook_ablate\n",
    "    \n",
    "    def assess(self, \n",
    "               data: pd.DataFrame,\n",
    "               mask: Optional[np.ndarray] = None):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm.model.to(device)\n",
    "        self.clear_hooks()\n",
    "\n",
    "        if mask is not None:\n",
    "            for idx, layer in enumerate(self.llm.model.model.layers):\n",
    "                layer.register_forward_hook(self.get_hook_ablate(idx, mask))\n",
    "\n",
    "        exp_df = data.copy()\n",
    "        max_num_tokens = exp_df[\"last_tokens\"].max()\n",
    "\n",
    "        last_tokens_embeddings = []\n",
    "        last_decoded_tokens = []\n",
    "        all_final_positions = []\n",
    "\n",
    "        for i in tqdm(range(0, len(exp_df), self.batch_size)):\n",
    "            batch_texts = exp_df[\"fusion\"].tolist()[i:i+self.batch_size]\n",
    "            # Tokenize the batch of texts and get input IDs\n",
    "            inputs = self.llm.tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            last_token_positions = (inputs[\"attention_mask\"].sum(dim=1) - 1)\n",
    "            # Define a range of offsets for the last tokens\n",
    "            expanded_positions = last_token_positions[:, None] - torch.arange(max_num_tokens, -1, -1, device=device)\n",
    "            token_positions = torch.gather(inputs[\"input_ids\"], 1, expanded_positions)\n",
    "            batch_indices = torch.arange(inputs[\"input_ids\"].shape[0]).unsqueeze(1).expand(-1, max_num_tokens+1)\n",
    "            outputs_softmax = torch.softmax(outputs.logits, dim=-1)\n",
    "            outputs_surprisal = -torch.log(outputs_softmax + 1e-8)\n",
    "            batch_surprisal = outputs_surprisal[batch_indices, expanded_positions, token_positions]\n",
    "\n",
    "            batch_decoded_tokens = [tokenizer.convert_ids_to_tokens(row.tolist()) for row in token_positions]\n",
    "\n",
    "            last_tokens_embeddings.extend(batch_surprisal.cpu().numpy())\n",
    "            last_decoded_tokens.extend(batch_decoded_tokens)\n",
    "            all_final_positions.extend(token_positions.cpu().numpy())\n",
    "\n",
    "        exp_df[\"last_embds\"] = last_tokens_embeddings\n",
    "        exp_df[\"last_decoded_tokens\"] = last_decoded_tokens\n",
    "        exp_df[\"last_pos\"] = all_final_positions\n",
    "        exp_df[\"score_cand\"] = exp_df.apply(compute_cand_score, axis=1)\n",
    "\n",
    "        # Group by 'id_prompt' and find the row with the maximum 'logits' for each group\n",
    "        best_cand_df = exp_df.loc[exp_df.groupby('id_prompt')['score_cand'].idxmax()]\n",
    "        # Reset the index if necessary\n",
    "        best_cand_df = best_cand_df.reset_index(drop=True)\n",
    "        subset = best_cand_df[[\"id_prompt\", \"cand\"]]\n",
    "        return subset\n",
    "    \n",
    "    def experiment(self, \n",
    "                   bn_data: BenchmarkToMi,\n",
    "                   pct=0.01):\n",
    "        assess_dict = {\n",
    "            \"no_ablation\": None,\n",
    "            f\"ablate_top_{int(pct * 100)}\": self.loc_units.get_masked_ktop(pct).T,\n",
    "            # f\"ablate_random1_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random2_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T,\n",
    "            # f\"ablate_random3_{int(pct * 100)}\": self.loc_units.get_random_mask(pct).T \n",
    "        }\n",
    "        df = bn_data.data.copy()\n",
    "        df = df.reset_index()\n",
    "        for key, mask in assess_dict.items():\n",
    "            subset = self.assess(bn_data.expanded_df, mask)\n",
    "            df = df.merge(subset, left_on=\"index\", right_on=\"id_prompt\", how=\"left\")\n",
    "            df = df.rename(columns={\"cand\": f\"predict_{key}\"})\n",
    "            df = df.drop(columns=[\"id_prompt\"]) \n",
    "        return df\n",
    "\n",
    "\n",
    "bn_tomi = BenchmarkToMi()\n",
    "bn_assess = AssessBenchmark(llm, loc_units)\n",
    "res = bn_assess.experiment(bn_tomi)\n",
    "res.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"checkpoint/result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
